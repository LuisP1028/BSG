<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Bayesian Modeling & Computation: Study Guide</title>
<style>
    /* 
       --- EDITABLE CHUNK START --- 
       Modify these variables to change the color scheme.
    */
    :root {
        --bg-color: #000000;           /* Pure Black Background */
        --text-color: #00ff41;         /* Terminal Green Text */
        --accent-color: #00ff41;       /* Bright Green Accents */
        --dim-color: #003b00;          /* Dark Green for subtle borders */
        --border-color: #00ff41;       /* Green Borders */
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden; /* Prevents horizontal scroll on mobile */
    }

    /* --- DITHERPUNK VISUALS (Green Mode) --- */
    
    /* Background Dither Simulation */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        /* Updated to Green dither dots */
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    /* Scanline Overlay */
    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        /* Green-tinted scanlines */
        background: linear-gradient(
            to bottom, 
            rgba(0, 255, 65, 0), 
            rgba(0, 255, 65, 0) 50%, 
            rgba(0, 20, 0, 0.2) 50%, 
            rgba(0, 20, 0, 0.2)
        );
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    /* --- RESPONSIVE CONTAINER --- */
    .container {
        max-width: 900px;         /* Limits width on large screens */
        width: 100%;              /* Ensures it takes full width on small screens */
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9); /* Very dark green background tint */
        min-height: 100vh;
    }

    /* Typography */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 3rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        letter-spacing: -2px;
        text-shadow: 0px 0px 8px var(--accent-color); /* Green Glow */
        color: var(--accent-color);
        text-align: center;
        word-wrap: break-word; /* Prevents long words breaking layout */
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }

    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION SYSTEM --- */

    /* PART LEVEL (Outer) */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    
    details.part[open] {
        box-shadow: 4px 4px 0px var(--dim-color);
        transform: translate(2px, 2px);
    }

    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color); /* Black text on Green background */
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }

    details.part > summary::-webkit-details-marker { display: none; }
    
    details.part > summary::after {
        content: '+'; 
        position: absolute; right: 20px; font-weight: 900;
    }
    details.part[open] > summary::after { content: '-'; }

    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }
    
    .focus-line {
        display: block;
        font-size: 0.8rem;
        text-transform: uppercase;
        letter-spacing: 2px;
        margin-bottom: 20px;
        color: #008f11;
        font-weight: bold;
    }

    /* SECTION LEVEL (Middle) */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }

    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }

    details.section > summary:hover { 
        background: var(--dim-color); 
        color: var(--accent-color); 
    }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }

    .section-content { padding: 20px; }

    /* SUBSECTION / CONTENT BLOCKS */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }

    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    /* Code/Math styling */
    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto; /* Ensures horizontal scrolling on small screens */
        white-space: pre-wrap; /* Wraps text if possible */
    }

    /* --- RESPONSIVE ADJUSTMENTS --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; } /* Justify can look weird on mobile */
    }
    /* --- END EDITABLE CHUNK --- */
</style>
</head>
<body>

<div class="dither-layer"></div>
<div class="scanlines"></div>

<div class="container">
    <h1>Bayesian Modeling<br>& Computation</h1>

    <!-- PART I -->
    <details class="part">
        <summary>Part I: Bayesian Foundations</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Concepts, MCMC, Priors</span>

            <!-- SECTION 1 -->
            <details class="section">
                <summary>1. Key Bayesian Concepts</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Prior</span>
                        <p>The prior distribution represents initial beliefs about model parameters before observing any data. It encodes pre-existing knowledge or assumptions about parameter values.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Observed data</span>
                        <p>Observed data are the actual measurements, used to update prior beliefs via the likelihood function in Bayesian inference.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Prior predictive distribution</span>
                        <p>The distribution of data that the model would generate based solely on the prior beliefs about parameters, before observing any actual data.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Marginal likelihood</span>
                        <p>The probability of the observed data under the model, computed by integrating the likelihood over the prior (also called model evidence).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Computing expectations</span>
                        <p>The average outcome of a quantity (like a parameter or prediction) under the posterior or predictive distribution.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 2 -->
            <details class="section">
                <summary>2. MCMC & Metropolis Hastings</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Markov Chain Monte Carlo (MCMC)</span>
                        <p> MCMC approximates parameter specifications t builds a chain of samples that maps the posterior distribution's shape and uncertainty.
                            .</p>
                        <p>It builds a Markov chain: starting from an initial point, it repeatedly proposes new "guesses" from a proposal distribution and decides whether to accept them, eventually producing samples that follow the target posterior.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Metropolis Hastings [ACCEPTING/REJECTING SAMPLES]</span>
                        <p><strong>1. The Current Stance:</strong> The algorithm is currently standing at a specific parameter value (let's call it Point A). It knows the "elevation" of this point by calculating the posterior density (Likelihood x Prior).</p>
                        <p><strong>2. The Proposal (The "What if?"):</strong> The algorithm blindly suggests a new spot nearby, usually by picking a random coordinate close to Point A. Let's call this Point B.</p>
                        <p><strong>3. The Comparison:</strong> It compares the elevation of Point B to Point A.</p>
                        <ul>
                            <li><strong>Case 1: Point B is higher (More Probable).</strong> If the posterior density at B is higher than at A, the move is a no-brainer. The algorithm Accepts immediately and moves to Point B.</li>
                            <li><strong>Case 2: Point B is lower (Less Probable).</strong> This is the genius part. It doesn't automatically reject the move. Instead, it calculates the ratio of the heights. If Point B is 70% as high as Point A, the algorithm rolls a virtual 10-sided die.
                                <ul>
                                    <li>If the roll is 7 or lower (70% chance), it Accepts and moves to the lower ground anyway.</li>
                                    <li>If the roll is 8, 9, or 10, it Rejects the move and stays at Point A.</li>
                                </ul>
                            </li>
                        </ul>
                        <p><strong>4. Recording the Sample:</strong></p>
                        <ul>
                            <li>If it Accepted: Point B becomes the new current position, and Point B is added to the list of samples.</li>
                            <li>If it Rejected: The hiker stays at Point A, and Point A is added to the list of samples again.</li>
                        </ul>
                        <p><strong>Why accept "worse" spots?</strong> If the hiker only ever moved uphill, they would get stuck on the top of the first small hill they found (a local maximum) and never discover the Mount Everest (global maximum) across the valley. By occasionally accepting downhill moves, the algorithm ensures it can traverse the valleys to find the true peaks of the posterior distribution.</p>
                    </div>
                </div>
            </details>



            <!-- SECTION 4 -->
            <details class="section">
                <summary>3. Types of Priors</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Conjugate Priors</span>
                        <p>"Same family in, same family out."</p>
                        <p>‚ÄúA prior is conjugate to a likelihood if the posterior belongs to the same family of distributions as the prior.‚Äù</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Objective Priors</span>
                        <p>‚ÄúIf you do not have information about a problem, then you do not have any reason to believe one outcome is more likely than any other.‚Äù</p>
                        <p><strong>Reparametrization:</strong> Describing the exact same system using different variables.<br>
                        </p>
                        <p><strong>JEFFREY‚ÄôS PRIOR:</strong> Identifies sensitive parameter values. The "Gold Standard" for 1D problems (estimating one thing).<br>
                        Invariant under reparametrization.<br>
                        
                        <p><strong>REFERENCE PRIORS:</strong> Maximize Kullback-Leibler Divergence; measures distance between posterior and prior distributions, with more distance being better [higher information distance]. By not encoding any bias on the prior, this lets the data speak as we collect more samples.<br>
                        KL Divergence measures the "information distance" between two distributions, P(x) (posterior) and Q(x) (prior)</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Maximum Entropy (MaxEnt) Priors</span>
                        <p>When we aren't totally clueless about a parameter's plausible values, we can use Maximum Entropy (MaxEnt) priors. By selecting the flattest prior possible that obeys your constraints, you maximize you admitted uncertainty.</p>
                        <p>For example, in modeling interest rates, a half-normal prevents negative values. By maximizing entropy, we allow the data to speak for itself.</p>
                        <p><strong>LAGRANGIAN MULTIPLIERS</strong><br>
                        <p>The math says: the probability at any point is just an exponential decay of how much that point violates your constraint(s).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Prior Predictive Distribution</span>
                        <p>It‚Äôs a sanity check. You draw parameters from your prior, then simulate data from them. If this "fake" data looks impossible‚Äîlike negative heights or million-degree days‚Äîyou know your prior is unrealistic and needs tightening.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

    <!-- PART II -->
    <details class="part">
        <summary>Part II: Diagnostics & Workflow</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Analysis, Convergence, HMC, Comparison</span>

            <!-- SECTION 5 -->
            <details class="section">
                <summary>1. Exploratory Analysis</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Bayesian Test Statistic (T-Stat)</span>
                        
                        <p>In Bayesian posterior predictive checks (PPCs), the "T" in diagrams (often labeled as T(y) or similar) isn't usually the Student's t-test; it‚Äôs a placeholder for "Test Statistic." It refers to any metric you choose to stress-test your model.</p>
                        <p>You define the test statistic based on what you suspect the model is missing.<br>
                        
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Bayesian P-Values</span>
                        
                        <p>p_B is simply the proportion of simulations where T_sim > T_obs.</p>
                        <p>An ideal p_B is ‚âà0.5, meaning the real data sits comfortably in the middle of the model's predictions. Values near 0 or 1 indicate systematic bias.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 6 -->
            <details class="section">
                <summary>2. Convergence: R-Hat, ESS, MCSE</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">1. R Hat: Convergence Factor [Gelman-Rubin]</span>
                        <p>MCMC methods have theoretical guarantees of correctness regardless of starting point, but only for infinite samples. In practice, we need ways to assess convergence for finite samples.</p>
                        <p>ÀÜR compares the spread between different chains to the spread within them. If ÀÜR > 1.01, your chains are effectively "disagreeing" or haven't mixed yet ‚Äî the simulation isn't finished, so don't trust the results.</p>
                        <p><strong>2. ESS (Effective Sample Size)</strong><br>
                        Check this second to measure precision.<br>
                        Once chains have converged (ÀÜR ‚âà 1), ESS tells you the volume of useful information in your samples.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">2. EFFECTIVE SAMPLE SIZE (ESS)</span>
                        <p>You use MCMC samples' statistical properties to find the ESS.</p>
                        <p><strong>The Calculation Steps</strong><br>
                        Since the true autocorrelations œÅ_k are unknown, they are estimated from the samples themselves:</p>
                        <p>1. Generate Samples<br>
                        Run your MCMC algorithm</p>
                        <p>2. Calculate Autocorrelation<br>
                        Compute the empirical autocorrelation œÅÃÇ_k for different lags k = 1, 2, 3, </p>
                        <p>3. Sum the Correlations (Truncation)<br>
                        <p>Standard software (like Stan, ArviZ, or R's coda) truncates the sum when the autocorrelation drops below a certain threshold. This step is crucial to filter out noise.</p>
                        <p>4. Final Division<br>
                        Divide the total number of samples N by this summed quantity (œÑ) to obtain the effective sample size.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">3. MCSE (Monte Carlo Standard Error)</span>
                        <p>MCSE is the "simulation noise"‚Äîthe error introduced by using an approximate algorithm instead of exact math.</p>
                        <p>It‚Äôs calculated as:<br>
                        MCSE = SD / ‚àöESS<br>
                        Where SD is the posterior standard deviation and ESS is the effective sample size.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">MCMC Workflow Summary</span>
                        <p>1. ÀÜR (Gelman-Rubin statistic)<br>
                        Confirms chains have mixed (validity). Check if ÀÜR ‚âà 1 (ÀÜR ‚â§ 1.01 is safe).</p>
                        <p>2. ESS (Effective Sample Size)<br>
                        Measures the volume of independent information (quantity). Aim for ESS >400 or >1000 per chain for stable estimates.</p>
                        <p>3. MCSE (Monte Carlo Standard Error)<br>
                        Uses ESS to assess if your estimate is precise enough for your scientific question (quality).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Trace Plot: Monitoring Convergence at a glance</span>
                        <p>"Fuzzy caterpillar" is a great rule of thumb, but not fail-safe. It implies mixing, but watch for:<br>
                        1. Sticking: Subtle "flat shelves" mean the sampler froze.<br>
                        2. Local Traps: A chain can look stable while stuck in one peak, missing others.</p>
                        <p>Treat visuals as a sniff test. Always verify with R^ (to check consensus) and ESS (to check volume) to ensure the fuzz is real.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 7 -->
            <details class="section">
                <summary>3. Hamiltonian Monte Carlo (HMC)</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">How HMC Works Step by Step</span>
                        <p>HMC treats the posterior distribution like a physical terrain to explore it efficiently. Here is how the machinery works, step-by-step:</p>
                        <p><strong>1. The Landscape (Negative Log-Probability)</strong><br>
                        To apply physics to statistics, we treat the probability distribution as a "Potential Energy" field.<br>
                        ‚Ä¢ The Inversion: In physics, gravity pulls objects to the lowest energy state (the bottom of a valley). In statistics, we want the highest probability (the top of a peak). By taking the negative of the probability, we flip the map upside down: high-probability peaks become deep valleys that the sampler naturally "falls" into.<br>
                        ‚Ä¢ The Logarithm: We use the log-probability because computers struggle to multiply tiny probabilities without crashing (underflow). Logarithms turn difficult multiplication into safe addition, keeping the simulation numerically stable.</p>
                        <p><strong>2. Momentum (The Kick)</strong><br>
                        To move around this landscape, we give the sampler a physical "kick." We pair every parameter with an auxiliary momentum variable, sampled from a proposal distribution (typically a Gaussian).<br>
                        The shape of this distribution (the mass matrix) acts like a vehicle's suspension. If tuned correctly to the valley's geometry, it allows the sampler to traverse long distances and complex correlations efficiently, rather than stumbling blindly like standard random-walk samplers.</p>
                        <p>In practice, HMC spends a "warm-up" phase learning the shape of the valley to select the appropriate matrix:<br>
                        ‚Ä¢ Identity Matrix: The default setting. It assumes all parameters have the exact same scale and no correlations. It often struggles with real-world data (e.g., comparing "Income" to "Age").<br>
                        ‚Ä¢ Diagonal Matrix: The industry standard. It estimates the variance of each parameter independently. It effectively "squishes" long dimensions and stretches short ones to make the probability valley look like a round bowl.<br>
                        ‚Ä¢ Dense Matrix: The "heavy artillery" for highly correlated parameters. If the valley runs diagonally, a Dense matrix captures the covariance and rotates the entire coordinate system, turning a tight, diagonal ravine into a perfectly aligned bowl.</p>
                        <p><strong>3. Simulation (Leapfrog Integration)</strong><br>
                        We simulate the trajectory using a method called Leapfrog integration. The sampler trades Potential Energy for Kinetic Energy: it speeds up as it dives into high-probability valleys and slows down as it climbs low-probability hills. This allows it to explore the entire shape of the "bowl" (the posterior) rather than just sitting at the bottom.</p>
                        <p><strong>4. Acceptance (Energy Conservation)</strong><br>
                        In a perfect physical system, Total Energy (Potential + Kinetic) is conserved. However, because computer simulations use discrete time steps, tiny errors creep in. We compare the energy at the start and end of the trajectory. If the energy is roughly conserved, the move is accepted. This correction step ensures the resulting samples are statistically valid.</p>
                        <p><strong>5. Divergences (The "Check Engine" Light)</strong><br>
                        If the landscape is too treacherous‚Äîlike a "funnel" with a dangerously steep neck‚Äîthe simulation step size may fail to capture the curvature. When this happens, the physics break, and energy shoots to infinity.<br>
                        These divergent transitions are invaluable diagnostics. They usually cluster in specific problem areas, pinpointing exactly where the geometry is broken. This guides reparameterization: the process of mathematically rewriting variables to reshape the terrain without changing predictions. For instance, a "non-centered parameterization" decouples correlated variables, effectively unfolding a crumpled funnel into a smooth, flat plain. By seeing where divergences occur, you know exactly which variables to untangle to fix the model.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">NON CENTERED PARAMETRIZATION</span>
                        <p>Hierarchical models often create correlation "funnels" that trap samplers. Non-centered parameterization fixes this by modeling variables as standardized deviations rather than direct dependencies. This decouples the parameters, effectively "unfolding" the crumpled geometry into a smooth plain. HMC can then navigate the terrain without divergences.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 8 -->
            <details class="section">
                <summary>4. Comparing Models</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">P_Loo (Effective Parameters via LOO)</span>
                        <p><strong>Leave One Out Mechanics Reminder</strong><br>
                        LOO is just a procedure (the act of rotating data). It produces no numbers or results on its own without a metric (like ELPD, MSE, or Accuracy) to calculate during that procedure.<br>
                        You cannot "just LOO"‚Äîyou must LOO with a scorecard.</p>
                        <p><strong>STEPS</strong><br>
                        1. Hide y_i (leave it out).<br>
                        2. Train the model on the remaining n-1 points.<br>
                        3. Score the log probability of the hidden y_i.<br>
                        Repeat this n times (once for every point) and sum the scores.<br>
                        This removes the "double-dipping" bias, providing a realistic measure of how well the model generalizes to new data rather than memorizing the training set.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Pareto Shape Parameter</span>
                        <p><strong>IDENTIFYING OVER-EMPHASIZED DATAPOINTS</strong><br>
                        PSIS-LOO: Pareto Shape Parameter<br>
                        In PSIS-LOO (Pareto Smoothed Importance Sampling Leave-One-Out), the shape parameter estimates how much the model relies on a single data point.</p>
                        <p>- Low Pareto Values<br>
                        The data point is normal; the model understands it easily alongside the others.</p>
                        <p>- High Pareto Values (> 0.7):<br>
                        The data point is highly influential and "surprising." The model struggles to fit it and is likely overfitting. Removing this point would drastically change predictions. It also signals that the posterior is too heavy-tailed for reliable approximation.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">p_loo: Effective Number of Parameters</span>
                        <p>p‚Çó‚Çí‚Çí measures the gap between in-sample fit (memorization) and out-of-sample generalization.</p>
                        <p><strong>Formula</strong><br>
                        p‚Çó‚Çí‚Çí = lppd ‚àí elpd‚Çó‚Çí‚Çí<br>
                        - lppd: Log pointwise predictive density on training data (overly optimistic due to overfitting/"double-dipping").<br>
                        - elpd‚Çó‚Çí‚Çí: Honest out-of-sample score via LOO cross-validation.</p>
                        <p><strong>Interpretation</strong><br>
                        p‚Çó‚Çí‚Çí represents the model's "optimism" (overfitting tendency).<br>
                        - Ideal: p‚Çó‚Çí‚Çí ‚âà actual number of parameters (e.g., ~5 for 5 variables).<br>
                        - Very large p‚Çó‚Çí‚Çí: Model is overly flexible and overfitting noise heavily.<br>
                        - p‚Çó‚Çí‚Çí >> actual parameters: Model relies too much on influential data points (high leverage) or the prior is too weak.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

    <!-- PART III -->
    <details class="part">
        <summary>Part III: Advanced Models</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Lasso, Statespace, Kalman Filter</span>

            <!-- SECTION 9 -->
            <details class="section">
                <summary>1. Laplace Priors (Bayesian Lasso)</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">The "Skeptical" Distribution</span>
                        <p>"Irrelevant until proven Essential."</p>
                        <p><strong>How it works as a test:</strong> When you aren't sure if a variable (like a specific sensor, a gene, or a market indicator) is predictive, you assign it a Laplace Prior. This forces the model to play a game of "King of the Hill."<br>
                        1. The Prior pulls every single variable toward zero (meaning: "this variable has no effect").<br>
                        2. The Data fights back. If a variable effectively lowers the error rate, the Data pulls it away from zero.<br>
                        3. The Result: Because the Laplace peak is so sharp, weak variables slide all the way down to exactly zero and "die." Only the variables with genuine, strong predictive power can survive the pull and remain non-zero.</p>
                        <p><strong>When to use it:</strong><br>
                        ‚Ä¢ High-Dimensional "Fishing": You have 1,000 potential variables but suspect only 5 actually matter.<br>
                        ‚Ä¢ Expensive Data: When keeping a variable "active" costs money (e.g., requires keeping a sensor online), you want to zero out the useless ones.<br>
                        ‚Ä¢ Interpretability: You need to explain to a human why the prediction happened (it's easier to point to 3 active variables than 1,000 slightly active ones).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Sector Examples</span>
                        <p><strong>1. Infrastructure: Smart Grid Load Balancing</strong><br>
                        The Unknown: You are managing a city's power grid. You have thousands of potential data inputs that might predict a power surge: temperature, humidity, wind speed, the time of day, the schedule of the local football stadium, the stock market opening bell, or school holidays. You don't know which combination actually drives demand.<br>
                        The Application: You feed all these inputs into a regression model with Laplace Priors on their coefficients.<br>
                        The Result: The model zeros out "Wind Speed" and "Stock Market" because their connection to power usage is weak or random. However, "Temperature" (AC usage) and "Stadium Schedule" (floodlights) survive the Laplace filter with large coefficients.<br>
                        ‚Ä¢ Outcome: You stop paying for the expensive stock market data feed because the Laplace Prior proved it had zero predictive value for the grid, saving money and simplifying the dashboard for operators.</p>
                        <p><strong>2. Defense: Acoustic Signature Classification (Sonar)</strong><br>
                        The Unknown: A submarine's passive sonar picks up a complex noise in the water. This sound is a mix of thousands of frequencies. You need to know if this is a whale, a merchant ship, or an enemy submarine. Most frequencies are just "ocean background noise," but you don't know which frequencies contain the hidden mechanical hum of a propeller.<br>
                        The Application: You treat every frequency band (Hz) as a variable. You apply a Laplace Prior to the amplitude weights of these bands.<br>
                        The Result: The Laplace Prior aggressively suppresses the thousands of frequencies related to waves and shrimp (background noise), forcing their weights to zero. It leaves behind a "sparse" set of specific non-zero frequencies‚Äîperhaps a harmonic series at 50Hz, 100Hz, and 150Hz.<br>
                        ‚Ä¢ Outcome: This sparse "fingerprint" is isolated from the noise. The system instantly recognizes this specific harmonic pattern as the signature of a specific class of diesel-electric engine, ignoring the rest of the ocean's chaotic noise.</p>
                        <p><strong>3. Healthcare: Drug Toxicity Prediction</strong><br>
                        The Unknown: You are developing a new drug and want to know if it will cause liver damage. You have data on the chemical structure of the drug, represented by thousands of molecular descriptors (number of carbon rings, bond angles, electrostatic charges, etc.). You don't know which structural feature causes the toxicity.<br>
                        The Application: You model the toxicity score using these thousands of molecular features, placing a Laplace Prior on the influence of each feature.<br>
                        The Result: The model discards 98% of the chemical features, setting them to zero. It reveals that toxicity is driven almost exclusively by the presence of a specific "nitrogen ring" structure combined with high solubility.<br>
                        ‚Ä¢ Outcome: Instead of telling chemists "change the molecule," you can tell them specifically: "Remove the nitrogen ring." The Laplace Prior turned a prediction problem into a design instruction by isolating the specific "toxic" variable</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Hamiltonian Monte Carlo + LaPlace Priors = Bayesian Lasso</span>
                        <p>HMC uses gradients to navigate parameter space like a skateboarder in a half-pipe. A Laplace Prior modifies this pipe, carving a deep, narrow trench at zero.</p>
                        <p>[[As HMC samples, it naturally slides the coefficients of irrelevant variables into this trench, trapping their probability mass near zero.<br>
                        However, essential features possess enough "data energy" (evidence) to push the skateboarder up the walls, keeping their values away from zero.]]</p>
                        <p>This combination (often called the Bayesian Lasso) is powerful because HMC explores high-dimensional spaces efficiently. It doesn't just select features; it quantifies certainty. You get a histogram for every variable: if the samples pile up at zero, it‚Äôs noise; if they hover strictly away from zero, the feature is essential.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 10 -->
            <details class="section">
                <summary>2. Statespace Modeling</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">State Space Modeling</span>
                        <p>It is a mathematical framework used to track a system that changes over time, where you cannot measure the variables you actually care about.</p>
                        <p>Imagine a dynamic system where a latent state ùê±‚Çú evolves from ùê±‚Çú‚Çã‚ÇÅ via a transition model, strictly adhering to the Markov property.<br>
                        Since ùê±‚Çú is inaccessible, we rely on observations ùê≤‚Çú‚Äînoisy emissions generated conditionally by the current state.</p>
                        <p>Effectively, it is the statistical reconstruction of the invisible trajectory of ùê± using only the imperfect measurements of ùê≤.</p>
                        <p><strong>Latent State:</strong> Information not directly observable, but that can be approximated using transformed data. We infer or estimate its probability distribution by conditioning on the history of observations ùê≤.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">1. FILTERING</span>
                        <p>It is a rhythm of Prediction and Correction.<br>
                        It separates the world into two layers:<br>
                        The Hidden Layer (ùê±‚Çú): The "True State." This is reality. (e.g., The exact coordinate of a missile, the true volatility of a stock, or the actual temperature of a nuclear core). You never see this.<br>
                        The Observation Layer (ùê≤‚Çú): The "Noisy Data." This is what your sensors tell you. (e.g., A blurry radar blip, a fluctuating stock price, or a thermometer reading with a margin of error).</p>
                        <p><strong>How does it work?</strong><br>
                        The model operates in a continuous loop called Filtering. It is a rhythm of Prediction and Correction.</p>
                        <p><strong>Best illustrated with an example</strong><br>
                        Imagine you are driving into a tunnel and your GPS loses signal.<br>
                        Predict (The Physics): Even without signal, the GPS assumes you are still moving forward at 60mph. It uses the previous state (ùê±‚Çú‚Çã‚ÇÅ) to predict the current state (ùê±‚Çú).<br>
                        Update (The Correction): Suddenly, you exit the tunnel and the satellite gets a ping (ùê≤‚Çú). The ping says you are 10 meters back from where the model guessed.<br>
                        Filter: The model combines its guess with the new data. If the satellite signal is usually noisy/unreliable, it trusts the guess more. If the signal is precise, it trusts the data more. The result is the Filtered Estimate.</p>
                        <p><strong>What is the "Marginal Distribution"?</strong><br>
                        This answers the question: "Where are we right now?"<br>
                        In probability, a "joint distribution" would try to calculate the probability of the entire path you took from time 0 to today. That is computationally heavy.<br>
                        The marginal distribution (ùëù(ùê±‚Çñ‚à£ùê≤‚ÇÄ:‚Çñ)) discards the history of how you got there. It focuses all its probability mass on defining the uncertainty of your location at this exact moment, conditioned on all past data.</p>
                        <p><strong>When to use it?</strong><br>
                        You use this whenever you need to estimate a variable that is dynamic (changing) and indirectly observed (noisy).<br>
                        Robotics/GPS: Your phone uses this (often a Kalman Filter). The GPS sensor is accurate to ~10m, but your blue dot on maps moves smoothly. That smooth movement is the hidden state ùê± being filtered from the noisy GPS data ùê≤.<br>
                        Finance: You observe stock prices (ùê≤), but you want to trade based on "market volatility" (ùê±). Volatility isn't a number you can download; you must infer it from the price swings.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">2. SMOOTHING</span>
                        <p>You aren't predicting tomorrow; you are refining the history.<br>
                        Smoothing is the "hindsight" engine. While Filtering is about knowing where you are now based on the past, Smoothing is about figuring out where you were yesterday, knowing what you know today.<br>
                        It computes the probability of a past state ùê±‚Çñ using the entire dataset ùê≤‚ÇÄ:‚Çú‚Äîmeaning it uses observations from before, during, and after the specific moment you are analyzing.</p>
                        <p><strong>What does it do?</strong><br>
                        It refines the history. Because it has access to "future" data (relative to the state ùê±‚Çñ being estimated), it eliminates false moves and noise that the Filter fell for in real-time.<br>
                        If Filtering is "Live Reporting," Smoothing is the "Post-Game Analysis." It connects the dots to create the most probable coherent trajectory of the system after the event has concluded.</p>
                        <p><strong>How does it work?</strong><br>
                        It is typically a two-pass process, often called the Forward-Backward algorithm:<br>
                        Forward Pass (Filtering): You run the standard filter from time 0 to ùëá. You make your best guesses in real-time.<br>
                        Backward Pass (Smoothing): You start at the end (time ùëá) and walk backwards to time 0. You use the information from the end of the sequence to correct the estimates in the middle.</p>
                        <p><strong>The Analogy: Imagine reading a messy handwritten sentence.</strong><br>
                        * Filtering: You read word-by-word. You see a squiggle and guess it says "cat."<br>
                        * Smoothing: You finish the sentence. The end of the sentence talks about "tires" and "engines." You go back and realize the squiggle was actually "car." The future context fixed the past interpretation.</p>
                        <p><strong>When to use it?</strong><br>
                        You use Smoothing for retrospective analysis or offline processing, where you have recorded data and need the highest accuracy possible, not real-time responsiveness.<br>
                        * Climate Science: Reconstructing past temperatures (ùê±) using ice cores and tree rings (ùê≤). You aren't predicting tomorrow; you are refining the history of the last 100 years.<br>
                        * Economics: Dating the start and end of a recession. It is essentially impossible to know you are in a recession the day it starts (Filtering), but extremely easy to pinpoint the start date two years later (Smoothing).<br>
                        * Training Models: When teaching a computer to understand a dynamic system (Parameter Learning), you use smoothed estimates to train it on "what actually happened."</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 11 -->
            <details class="section">
                <summary>3. Linear Gaussian SSM</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Explained through a missile navigation system</span>
                        <p><strong>1. The State Evolution Equation</strong><br>
                        <span class="code-block">ùêó‚Çú = ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ + ùõà‚Çú</span><br>
                        This equation defines the "physics" of the hidden system.<br>
                        * ùêó‚Çú (State Vector): This is not a single number, but a column of numbers (a vector) representing the complete internal status of the system. For a moving object, this vector might contain [position, velocity, acceleration]. It holds all dimensions required to predict the future, even those not directly visible.<br>
                        * ùêÖ‚Çú (Transition Matrix): This matrix is the "engine" of the system. It is a linear operator that multiplies the previous state vector (ùêó‚Çú‚Çã‚ÇÅ) to calculate the new state. It encodes the rules of interaction between state variables‚Äîfor example, it mathematically enforces that "current position = previous position + previous velocity."<br>
                        * ùõà‚Çú (Process Noise Vector): A vector of random shocks representing external realities not captured by your physics engine (e.g., wind gusts or wheel slip).<br>
                        * êêê‚Çú (Process Covariance Matrix): This matrix defines the geometry of the uncertainty in the system dynamics. The numbers on the diagonal tell you the magnitude (variance) of the shocks for each variable. The off-diagonal numbers tell you the correlations‚Äîif the velocity gets a random shock, does the position also tend to get a shock?</p>
                        <p><strong>2. The Observation Equation</strong><br>
                        <span class="code-block">ùêò‚Çú = ùêá‚Çú ùêó‚Çú + ùõÜ‚Çú</span><br>
                        This equation defines the "lens" through which we view the system.<br>
                        * ùêò‚Çú (Observation Vector): The raw data collected by your sensors at time ùë°. This vector typically has fewer dimensions than the state vector (e.g., a GPS sensor reads [position], but cannot read [velocity]).<br>
                        * ùêá‚Çú (Observation Matrix): This matrix describes the relationship between the hidden truth and the sensor reading. It acts as a projection operator, mapping the high-dimensional "State Space" down to the lower-dimensional "Measurement Space." It essentially selects or combines parts of ùêó‚Çú to simulate what the sensor should see.<br>
                        * ùõÜ‚Çú (Measurement Noise Vector): A vector representing the random error or "static" inherent in the sensors.<br>
                        * ùêë‚Çú (Measurement Covariance Matrix): This matrix quantifies the reliability of your sensors. A high value on the diagonal means that specific sensor is very noisy (high variance); a low value means it is precise. The Kalman Filter uses this matrix to decide how much to "trust" the incoming data ùêò‚Çú versus its own prediction from Step 1.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Operational Interactions: The Mechanics of the Matrix</span>
                        <p>How these mathematical components mechanically interact to simulate the system.</p>
                        <p><strong>1. The State Update: Linear Combination and Displacement</strong><br>
                        [[[ ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ + ùõà‚Çú<br>
                        The product of the transition matrix [ùêÖ‚Çú] and the previous values of the state vector [ùêó‚Çú‚Çã‚ÇÅ] is used to ensure the transformation rules are being enforced. ]]]<br>
                        ‚ñ¨ŒπìÜÉ<br>
                        ‚Ä¢ The Matrix-Vector Product (ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ): This operation functions as a "mixing board." In linear algebra, multiplying a matrix by a vector creates a linear combination of the inputs.<br>
                        Consider a simple physics example where the state vector contains [Position, Velocity].<br>
                        The transition matrix [ùêÖ‚Çú ]contains the "rules"<br>
                        (Position‚Çô‚Çë‚Çì‚Çú = Position‚Çí‚Çó‚Çî + Velocity‚Çí‚Çó‚Çî √ó Œîùë°).<br>
                        When ùêÖ‚Çú multiplies ùêó‚Çú‚Çã‚ÇÅ, it simultaneously calculates the new values for every row in the vector by taking the weighted sum of the previous values. It mechanically enforces the coupling between variables‚Äîensuring the new position is mathematically derived from the old velocity.<br>
                        ‚Ä¢ The Vector Addition (+ ùõà‚Çú): This operation performs a coordinate shift. After the matrix ùêÖ‚Çú predicts the perfect theoretical location of the system in the abstract state space, the addition of the noise vector ùõà‚Çú "nudges" that point off-target. It displaces the state coordinate randomly, simulating the external forces that the physics definitions ignored.</p>
                        <p>Here is a clarified explanation of the interaction between the Observation Matrix and the State Vector:</p>
                        <p><strong>2. The Observation Projection: Translation and Selection</strong><br>
                        [[[ ùêá‚Çú ùêó‚Çú<br>
                        The product of the obs matrix [ùêá‚Çú ] by the state vector [ùêó‚Çú]is to filter out latent variables. ]]]<br>
                        By performing the multiplication, you are essentially saying:<br>
                        "Take the reality of the past (ùêó‚Çú‚Çã‚ÇÅ) and force it through the logic of my physics engine (ùêÖ‚Çú) to calculate where it must be now."<br>
                        ‚ñ¨ŒπìÜÉ<br>
                        * The State Vector (ùêó‚Çú) is the "Hidden Truth." It contains every detail the system tracks to function mathematically (e.g., [Position, Velocity, Acceleration]).<br>
                        * The Observation Matrix (ùêá‚Çú) is the "Selector." It tells the equation which parts of that truth the sensor can actually see.</p>
                        <p><strong>How the Math Works (The "Dot Product"):</strong><br>
                        Imagine a simple vehicle tracker.<br>
                        * The State ùêó‚Çú has two rows: Position (top) and Velocity (bottom).<br>
                        * Your sensor is a GPS that only reads location; it is blind to speed.<br>
                        * Therefore, the Observation Matrix ùêá‚Çú is set to [1, 0].<br>
                        When ùêá‚Çú multiplies ùêó‚Çú, it performs a row-by-column calculation:<br>
                        (1 √ó Position) + (0 √ó Velocity) = Observed Position<br>
                        By multiplying Velocity by 0, the matrix mechanically deletes that information from the result. By multiplying Position by 1, it passes that information through unchanged.</p>
                        <p><strong>The Result:</strong><br>
                        This reduces the complex 2-dimensional state down to a 1-dimensional measurement (ùêò‚Çú).<br>
                        The matrix mathematically simulates the sensor's limitations, ensuring the Kalman Filter compares "apples to apples"‚Äîmatching its internal prediction of position against the actual GPS reading, while ignoring the velocity that the GPS cannot measure.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Generative Process of the Linear Gaussian State Space Model (SSM)</span>
                        <p>The generative process asks, "If the system works like this, what kind of data would it produce?"</p>
                        <p><strong>1. The Notation</strong><br>
                        First, let‚Äôs decode the notation that defines the relationship.<br>
                        ‚àº (Tilde): "Is sampled from" or "follows the distribution of." When we say ùêó ‚àº ùí©, we mean ùêó is a random variable generated by a Normal distribution.<br>
                        ‚à£ (Pipe): "Given." This indicates conditional probability. ùëù(ùêó‚Çú‚à£ùêó‚Çú‚Çã‚ÇÅ) asks: "What is the probability of the current state (ùêó‚Çú), given that we already know the previous state (ùêó‚Çú‚Çã‚ÇÅ)?"<br>
                        ‚â° (Triple Bar): "Is defined as" or "is equivalent to." This symbol links the abstract concept (the probability ùëù) to the specific mathematical implementation (the Normal distribution ùí©).<br>
                        ùí©(ùùÅ, Œ£) (Script N): Represents the Normal (Gaussian) Distribution. It requires two arguments: the Mean (ùùÅ, the center/peak) and the Covariance (Œ£, the width/spread).</p>
                        <p><strong>2. Equation 1: The Physics (Process Model)</strong><br>
                        <span class="code-block">ùêó‚Çú ‚àº ùí©(ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ, ùêê‚Çú)</span><br>
                        This equation says: "The State at time ùë° is a random draw from a Bell Curve."</p>
                        <p>A. Why is the Mean ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ?<br>
                        In a Normal distribution, the Mean represents the "perfect," "expected," or "most likely" value.<br>
                        The term ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ is your deterministic physics engine. It applies the transformation rules (e.g., position + velocity) to the previous state.<br>
                        If we lived in a perfect universe with no wind, friction, or entropy, the state would be exactly ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ.<br>
                        Therefore, we center the bell curve right on top of this physical prediction. It is our "best guess."</p>
                        <p>B. Why is ùêê‚Çú the Sigma (Covariance)?<br>
                        In a standard bell curve, Sigma (œÉ) defines the width. In this vector equation, ùêê‚Çú is the Covariance Matrix.<br>
                        This represents the "Process Noise." It accounts for external realities the physics engine ignored (wind gusts, wheel slips).</p>
                        <p><strong>3. Equation 2: The Sensor (Observation Model)</strong><br>
                        <span class="code-block">ùêò‚Çú ‚àº ùí©(ùêá‚Çú ùêó‚Çú, ùêë‚Çú)</span><br>
                        This equation says: "The Data reading at time ùë° is a random draw from a Bell Curve centered around the truth."</p>
                        <p>A. Why is the Mean ùêá‚Çú ùêó‚Çú?<br>
                        ùêá‚Çú is the "lens" or "mask" matrix. It converts the hidden state (Truth) into what the sensor should theoretically see.<br>
                        If the sensor were perfect, the reading ùêò‚Çú would equal ùêá‚Çú ùêó‚Çú exactly.<br>
                        Therefore, we center the probability distribution on this "Ideal Reading."</p>
                        <p>B. Why is ùêë‚Çú the Sigma (Covariance)?<br>
                        ùêë‚Çú represents Measurement Noise.<br>
                        This defines the quality of your hardware. A cheap sensor has a high ùêë (wide spread, fuzzy data). An expensive, precision sensor has a low ùêë (narrow spread, sharp data).</p>
                        <p><strong>4. Summary of Interactions</strong><br>
                        This system allows the Kalman Filter to mathematically weigh two competing uncertainties:<br>
                        The Physics Uncertainty (ùêê‚Çú): "How much do I trust my prediction?"<br>
                        The Sensor Uncertainty (ùêë‚Çú): "How much do I trust my data?"<br>
                        If ùêê is large and ùêë is small: The math "trusts" the data. The bell curve is wide for the prediction but narrow for the measurement, so the final estimate snaps to the measurement.<br>
                        If ùêê is small and ùêë is large: The math "trusts" the physics. The data is too noisy, so the filter ignores the data and sticks to the trajectory calculated by ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 12 -->
            <details class="section">
                <summary>4. Kalman Filter</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">The Mechanics</span>
                        <p>Think of the Kalman Gain as a dynamic mixing slider that moves automatically at every time step between 0 (Pure Physics) and 1 (Pure Data).</p>
                        <p><strong>1. The Mechanics of the Gain</strong><br>
                        The Gain compares the Predicted Uncertainty (derived from ùêê) against the Measurement Uncertainty (ùêë).<br>
                        <span class="code-block">ùêä‚Çú ‚âà Uncertainty_Predict / (Uncertainty_Predict + Uncertainty_Measure)</span></p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Scenarios</span>
                        <p><strong>2. Scenario A: High ùêê, Low ùêë (Trust the Data)</strong><br>
                        * The Situation: You are tracking an erratic drone (High Process Noise) with a laser precision tracker (Low Measurement Noise).<br>
                        * The Cycle: During the Predict step, the drone's erratic behavior makes the uncertainty "bubble" expand rapidly.<br>
                        * The Action: The math sees the Prediction is vague but the Sensor is sharp. The Kalman Gain shoots up (close to 1).<br>
                        * The Result: In the Update step, the filter aggressively snaps the state to the measurement, effectively resetting the prediction to match the laser.</p>
                        <p><strong>3. Scenario B: Low ùêê, High ùêë (Trust the Physics)</strong><br>
                        * The Situation: You are tracking a heavy freight train (Low Process Noise) with a fuzzy analog camera (High Measurement Noise).<br>
                        * The Cycle: During the Predict step, the train's massive inertia means the uncertainty stays tight‚Äîwe know exactly where it should be.<br>
                        * The Action: The math sees the Sensor is a "shotgun blast" of noise compared to the tight Physics prediction. The Kalman Gain drops (close to 0).<br>
                        * The Result: In the Update step, the filter treats the observation as "static." It barely nudges the state, allowing the smooth physics trajectory to dominate.</p>
                        <p><strong>Summary</strong><br>
                        The Kalman Filter aids the cycle by optimizing the correction. It prevents the system from chasing "ghosts" (sensor noise) while ensuring it doesn't become "blind" to real changes in direction.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

</div>

</body>
</html>