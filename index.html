<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Bayesian Modeling & Computation: Study Guide</title>
<style>
    /* 
       --- EDITABLE CHUNK START --- 
       Modify these variables to change the color scheme.
    */
    :root {
        --bg-color: #000000;           /* Pure Black Background */
        --text-color: #00ff41;         /* Terminal Green Text */
        --accent-color: #00ff41;       /* Bright Green Accents */
        --dim-color: #003b00;          /* Dark Green for subtle borders */
        --border-color: #00ff41;       /* Green Borders */
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden; /* Prevents horizontal scroll on mobile */
    }

    /* --- DITHERPUNK VISUALS (Green Mode) --- */
    
    /* Background Dither Simulation */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        /* Updated to Green dither dots */
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    /* Scanline Overlay */
    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        /* Green-tinted scanlines */
        background: linear-gradient(
            to bottom, 
            rgba(0, 255, 65, 0), 
            rgba(0, 255, 65, 0) 50%, 
            rgba(0, 20, 0, 0.2) 50%, 
            rgba(0, 20, 0, 0.2)
        );
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    /* --- RESPONSIVE CONTAINER --- */
    .container {
        max-width: 900px;         /* Limits width on large screens */
        width: 100%;              /* Ensures it takes full width on small screens */
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9); /* Very dark green background tint */
        min-height: 100vh;
    }

    /* Typography */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 3rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        letter-spacing: -2px;
        text-shadow: 0px 0px 8px var(--accent-color); /* Green Glow */
        color: var(--accent-color);
        text-align: center;
        word-wrap: break-word; /* Prevents long words breaking layout */
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }

    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION SYSTEM --- */

    /* PART LEVEL (Outer) */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    
    details.part[open] {
        box-shadow: 4px 4px 0px var(--dim-color);
        transform: translate(2px, 2px);
    }

    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color); /* Black text on Green background */
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }

    details.part > summary::-webkit-details-marker { display: none; }
    
    details.part > summary::after {
        content: '+'; 
        position: absolute; right: 20px; font-weight: 900;
    }
    details.part[open] > summary::after { content: '-'; }

    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }
    
    .focus-line {
        display: block;
        font-size: 0.8rem;
        text-transform: uppercase;
        letter-spacing: 2px;
        margin-bottom: 20px;
        color: #008f11;
        font-weight: bold;
    }

    /* SECTION LEVEL (Middle) */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }

    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }

    details.section > summary:hover { 
        background: var(--dim-color); 
        color: var(--accent-color); 
    }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }

    .section-content { padding: 20px; }

    /* SUBSECTION / CONTENT BLOCKS */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }

    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    /* Code/Math styling */
    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto; /* Ensures horizontal scrolling on small screens */
        white-space: pre-wrap; /* Wraps text if possible */
    }

    /* --- DITHERPUNK UI ADDITIONS --- */

    /* 1. Dither Pattern for Modal Background */
    .dither-bg {
        background-image: 
            linear-gradient(45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(-45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(45deg, transparent 75%, var(--dim-color) 75%), 
            linear-gradient(-45deg, transparent 75%, var(--dim-color) 75%);
        background-size: 4px 4px;
    }

    /* 2. The Eye Button */
    .eye-btn {
        position: relative;
        width: 28px;
        height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        margin-left: 8px;
        vertical-align: bottom; /* Aligns with text baseline better */
        transition: transform 0.1s;
    }

    .eye-btn svg {
        width: 100%; height: 100%;
        fill: var(--accent-color);
    }

    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    /* 3. The Popup Viewport */
    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh; /* Responsive size */
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both; /* Allow resizing */
        overflow: hidden;
    }

    .retro-viewport.active {
        visibility: visible; opacity: 1; pointer-events: auto;
    }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }

    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }

    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    /* --- RESPONSIVE ADJUSTMENTS --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; } /* Justify can look weird on mobile */
        .retro-viewport { width: 95vw; height: 60vh; }
    }
    /* --- END EDITABLE CHUNK --- */
</style>
</head>
<body>

<div class="dither-layer"></div>
<div class="scanlines"></div>

<div class="container">
    <h1>Bayesian Modeling<br>& Computation</h1>

    <!-- PART I -->
    <details class="part">
        <summary>Part I: Bayesian Foundations</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Concepts, MCMC, Priors</span>

            <!-- SECTION 1 -->
            <details class="section">
                <summary>1. Key Bayesian Concepts</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Prior</span>
                        <p>The prior distribution represents initial beliefs about model parameters before observing any data. It encodes pre-existing knowledge or assumptions about parameter values.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Observed data</span>
                        <p>Observed data are the actual measurements, used to update prior beliefs via the likelihood function in Bayesian inference.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Prior predictive distribution</span>
                        <p>The distribution of data that the model would generate based solely on the prior beliefs about parameters, before observing any actual data.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Marginal likelihood</span>
                        <p>The probability of the observed data under the model.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Computing expectations</span>
                        <p>The average outcome of a quantity (like a parameter or prediction) under the posterior or predictive distribution.</p>
                    </div>
                </div>
            </details>

<!-- SECTION 2 -->
<details class="section">
    <summary>2. MCMC & Metropolis Hastings</summary>
    <div class="section-content">
        <div class="subsection">
            <span class="subsection-title">
                Markov Chain Monte Carlo (MCMC)

                <!-- BUTTON START -->
                <button class="eye-btn" onclick="openViewport('metro_ess.html', 'MCMC VISUALIZATION')" title="Visualize MCMC">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                    </svg>
                </button>
                <!-- BUTTON END -->
                <p>MCMC approximates the posterior by building a chain of samples. It iteratively proposes and accepts guesses to map the posterior distribution's shape.</p>
                <p>The posterior is a mathematical function representing your updated beliefs.</p>

                <!-- Start of Inserted Explanation -->
                <div style="margin-top: 2rem;">
                    <p>The posterior probability formula looks like this:</p>
    
                    <!-- Equation 1: Basic Posterior -->
                    <div style="display: flex; align-items: center; justify-content: center; font-family: 'Times New Roman', serif; font-size: 1.2rem; margin: 1.5rem 0;">
                        <span style="margin-right: 10px;">Posterior =</span>
                        <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
                            <div style="border-bottom: 1px solid white; padding-bottom: 4px; margin-bottom: 4px;">
                                Likelihood &times; Prior
                            </div>
                            <div>
                                The Total Sum
                            </div>
                        </div>
                    </div>
    
                    <h3>The Problem</h3>
                    <p><strong>"The Total Sum"</strong> is the part we often cannot solve analytically. It requires adding up every possible version of the world (integrating over high-dimensional space), which is computationally impossible. Without it, we can‚Äôt calculate the absolute probability (e.g., "There is exactly a 12.5% chance").</p>
    
                    <h3>The Solution</h3>
                    <p>We rely on <strong>Relative Probability</strong>. This is the ratio that tells us how much more likely one specific guess is compared to another (e.g., "Point B is twice as likely as Point A"), without needing to know the probability of either point in isolation.</p>
    
                    <h3>The Trick</h3>
                    <p>When we compare two points (A and B) to decide where the sampler should step next, we divide their formulas. Because "The Total Sum" is the denominator for both A and B, it cancels out mathematically:</p>
    
                    <!-- Equation 2: The Cancellation -->
                    <div style="display: flex; flex-wrap: wrap; align-items: center; justify-content: center; font-family: 'Times New Roman', serif; font-size: 1.2rem; margin: 2rem 0; gap: 15px;">
                        
                        <!-- Term 1: Ratio of Posteriors -->
                        <div style="display: flex; flex-direction: column; align-items: center;">
                            <div style="border-bottom: 1px solid white; padding-bottom: 4px; margin-bottom: 4px;">Posterior of B</div>
                            <div>Posterior of A</div>
                        </div>
    
                        <span>=</span>
    
                        <!-- Term 2: Complex Fraction showing cancellation -->
                        <div style="display: flex; flex-direction: column; align-items: center;">
                            <!-- Numerator Side -->
                            <div style="border-bottom: 1px solid white; padding-bottom: 6px; margin-bottom: 6px; display: flex; flex-direction: column; align-items: center;">
                                 <div style="border-bottom: 1px solid white; font-size: 0.9em;">Numerator B</div>
                                 <div style="color: #ff4d4d; font-size: 0.8em;">The Total Sum</div>
                            </div>
                            <!-- Denominator Side -->
                            <div style="display: flex; flex-direction: column; align-items: center;">
                                 <div style="border-bottom: 1px solid white; font-size: 0.9em;">Numerator A</div>
                                 <div style="color: #ff4d4d; font-size: 0.8em;">The Total Sum</div>
                            </div>
                        </div>
    
                        <span>=</span>
    
                        <!-- Term 3: Final Result -->
                        <div style="display: flex; flex-direction: column; align-items: center;">
                            <div style="border-bottom: 1px solid white; padding-bottom: 4px; margin-bottom: 4px;">Numerator B</div>
                            <div>Numerator A</div>
                        </div>
    
                    </div>
    
                    <p>By using relative probability, we are left with only the Numerators (Likelihood &times; Prior). We can calculate these easily, allowing us to map the peaks and valleys of the distribution without ever knowing the mountain's total volume.</p>
                </div>
                <!-- End of Inserted Explanation -->
            </div>
        <div class="subsection">
            <span class="subsection-title">
                Metropolis Hastings [ACCEPTING/REJECTING SAMPLES]

                <!-- BUTTON START -->
                <button class="eye-btn" onclick="openViewport('metro_ess.html', 'METROPOLIS HASTINGS')" title="Visualize Metropolis-Hastings">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                    </svg>
                </button>
                <!-- BUTTON END -->
            </span>
            <p>Metropolis-Hastings acts like an explorer mapping a probability landscape. From its current spot (Point A), it suggests a random nearby step (Point B) and compares their posterior densities.</p>
            <ul>
                <li><strong>The Uphill Move:</strong> If Point B has a higher density (more probable), the algorithm always accepts and moves there.</li>
                <li><strong>The Downhill Gamble:</strong> If Point B is lower, it doesn't automatically reject it. Instead, it accepts the move with a probability based on the ratio of the two heights (e.g., if B is 70% as high as A, there is a 70% chance to move).</li>
                <li><strong>The Outcome:</strong> If accepted, B becomes the next sample; if rejected, it stays put and records A again. This willingness to occasionally step "downhill" prevents the algorithm from getting stuck on local peaks, ensuring it traverses valleys to fully map the distribution's true shape.</li>
            </ul>
        </div>
    </div>
</details>
    <!-- SECTION 4 -->
    <details class="section">
        <summary>3. Types of Priors</summary>
        <div class="section-content">
            <div class="subsection">
                <span class="subsection-title">Conjugate Priors</span>
                <p>"Same family in, same family out."</p>
                
            </div>
            <div class="subsection">
                <span class="subsection-title">Objective Priors</span>
                <p>‚ÄúIf you do not have information about a problem, then you do not have any reason to believe one outcome is more likely than any other.‚Äù</p>
                <p><strong>Reparametrization:</strong> Describing the exact same system using different variables.<br>
                </p>
                <p>
                    <strong>JEFFREY‚ÄôS PRIOR:</strong> 
                    
                    <!-- BUTTON START -->
                    <button class="eye-btn" onclick="openViewport('JP_warpsimple.html', 'INVARIANCE / WARPING')" title="Visualize Reparametrization">
                        <svg viewBox="0 0 24 24">
                            <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                            <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                        </svg>
                    </button>
                    <!-- BUTTON END -->
            
                    An objective prior invariant under reparametrization (consistent across units), making it the 1D "gold standard." It defines probability using Fisher Information, which measures likelihood sensitivity.
                </p>
                <p>Fisher Information: 
                    <!-- EYE BUTTON INSERTED HERE -->
                    <button class="eye-btn" onclick="openViewport('FISHER.html', 'FISHER INFO // POSTERIOR ANALYSIS')" title="Visualize Fisher Info">
                        <svg viewBox="0 0 24 24">
                            <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                            <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                        </svg>
                    </button>
                    <!-- END BUTTON -->It maps the "curvature" of the parameter space. It assigns more probability mass to high-sensitivity regions (where small nudges to the parameter drastically change the likelihood) and less to flat areas. This ensures the prior reflects information density rather than just scale.<br><br>
                    <p>
                        <strong>REFERENCE PRIORS:</strong>
                        
                        <!-- BUTTON START -->
                        <button class="eye-btn" onclick="openViewport('KL_Divergence.html', 'KL DIVERGENCE METRIC')" title="Visualize KL Divergence">
                            <svg viewBox="0 0 24 24">
                                <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                                <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                            </svg>
                        </button>
                        <!-- BUTTON END -->
                    
                        Maximize Kullback-Leibler Divergence; measures distance between posterior and prior distributions, with more distance being better [higher information distance]. By not encoding any bias on the prior, this lets the data speak as we collect more samples.<br>
                        KL Divergence measures the "information distance" between two distributions, P(x) (posterior) and Q(x) (prior)
                    </p>
                    </div>
            <div class="subsection">
                <span class="subsection-title">Maximum Entropy (MaxEnt) Priors</span>
                <p>When we aren't totally clueless about a parameter's plausible values, we can use Maximum Entropy (MaxEnt) priors. By selecting the flattest prior possible that obeys your constraints, you maximize you admitted uncertainty.</p>
                <p>For example, in modeling interest rates, a half-normal prevents negative values. By maximizing entropy, we allow the data to speak for itself.</p>
                <p><strong>LAGRANGIAN MULTIPLIERS</strong><br>
                <p>The math says: the probability at any point is just an exponential decay of how much that point violates your constraint(s).</p>
            </div>
            <div class="subsection">
                <span class="subsection-title">Prior Predictive Distribution</span>
                <p>It‚Äôs a sanity check. You draw parameters from your prior, then simulate data from them. If this "fake" data looks impossible‚Äîlike negative heights or million-degree days‚Äîyou know your prior is unrealistic and needs tightening.</p>
            </div>
        </div>
    </details>
</div>
</details>

    <!-- PART II -->
    <details class="part">
        <summary>Part II: Diagnostics & Workflow</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Analysis, Convergence, HMC, Comparison</span>

<!-- SECTION 5 -->
<details class="section">
    <summary>1. Exploratory Analysis</summary>
    <div class="section-content">
        <div class="subsection">
            <span class="subsection-title">Bayesian Test Statistic (T-Stat)</span>
            
            <p>Think of <em>ùíØ</em> as a "yardstick" chosen to measure a specific feature of the data, such as the average, the standard deviation, or the maximum value.</p>
            
            <ul style="margin-top: 5px; padding-left: 20px;">
                <li><strong>ùíØ_obs:</strong> The metric value measured from the real data.</li>
                <li><strong>ùíØ_sim:</strong> The metric value measured from fake data generated by the model.</li>
            </ul>
        </div>
        
        <div class="subsection">
            <span class="subsection-title">Bayesian P-Values</span>
            
            <p>This is simply a percentile ranking. It calculates the proportion of times the model's simulated data resulted in a higher metric value than in the real data.</p>
            
            <p style="text-align: center; margin: 10px 0; font-weight: bold;">
                ùëù_B = Proportion of (ùíØ_sim > ùíØ_obs)
            </p>
            
            <hr style="opacity: 0.3; margin: 10px 0;">
            
            <p><strong>How to Interpret:</strong></p>
            <ul style="margin-top: 5px; padding-left: 20px;">
                <li style="margin-bottom: 10px;">
                    <strong>ùëù_B ‚âà 0.5 (Ideal):</strong> The model fits well. The real data sits comfortably in the middle of the model's predictions, looking just like a typical simulation.
                </li>
                <li>
                    <strong>ùëù_B near 0 or 1 (Bias):</strong> The model is systematically failing.
                    <ul style="margin-top: 5px; padding-left: 20px; opacity: 0.9;">
                        <li><em>Near 1 (e.g., 0.99):</em> The model consistently predicts values higher than reality.</li>
                        <li><em>Near 0 (e.g., 0.01):</em> The model consistently predicts values lower than reality.</li>
                    </ul>
                </li>
            </ul>
        </div>
    </div>
</details>
<!-- SECTION 6 -->
<details class="section">
    <summary>2. Convergence: R-Hat, ESS, MCSE</summary>
    <div class="section-content">
        <div class="subsection">
            <span class="subsection-title">
                1. R Hat: Convergence Factor [Gelman-Rubin]
                
                <!-- BUTTON START -->
                <button class="eye-btn" onclick="openViewport('R_Hat.html', 'GELMAN-RUBIN STATISTIC')" title="Visualize R-Hat">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                    </svg>
                </button>
                <!-- BUTTON END -->
            </span>
            <p>MCMC methods have theoretical guarantees of correctness regardless of starting point, but only for infinite samples. In practice, we need ways to assess convergence for finite samples.</p>
            <p>ùëÖÃÇ^ calculates the "total variance" by combining two sources: the within-chain variance (the average spread inside each chain) and the between-chain variance (the variance of the chain means). ùëÖÃÇ is essentially the square root of the ratio of this total variance to the within-chain variance. If the chains have converged, they will be indistinguishable; the between-chain variance will be negligible, and the total variance will equal the within-chain variance, resulting in ùëÖÃÇ ‚âà 1.0. If ùëÖÃÇ &gt; 1.01, the chains are still distinguishing themselves from one another (the between-chain variance is significant), indicating they have not yet fully explored the target distribution.</p>
            </div>
        <div class="subsection">
            <span class="subsection-title">
                2. Effective Sample Size (ESS)
        
                <!-- BUTTON START -->
                <button class="eye-btn" onclick="openViewport('metro_ess.html', 'EFFECTIVE SAMPLE SIZE')" title="Visualize ESS">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                    </svg>
                </button>
                <!-- BUTTON END -->
            </span>
            <p>Check this second to measure precision.</p>
            <p>Once chains have converged (ÀÜR ‚âà 1), ESS tells you the volume of useful information in your samples.</p>
            <p><strong>What it is:</strong><br>
            ESS estimates the number of independent data points your MCMC chain is equivalent to. Because MCMC samples are correlated (‚Äústicky‚Äù), 4,000 steps might only provide as much information as 800 independent draws.</p>
            <p><strong>How it is computed:</strong><br>
            It penalizes the total sample count (N) based on autocorrelation.</p>
            <p>1. Measure Stickiness: Calculate the correlation between samples at various lags (œÅÃÇ_k).</p>
            <p>2. Sum & Divide: Sum the positive correlations to find the ‚Äúinefficiency factor,‚Äù then divide the total count by it.</p>
            <p>ESS = N / Sum of Autocorrelations</p>
            <p>High correlation ‚Üí High inefficiency ‚Üí Low ESS.</p>
            <p><strong>Why it is helpful:</strong><br>
            It measures precision.</p>
            <ul>
                <li>High ESS: Your posterior estimates (mean, intervals) are stable and accurate.</li>
                <li>Low ESS: Your estimates are noisy and unreliable, even if the model has converged (RÃÇ ‚âà 1).</li>
            </ul>
        </div>
        <div class="subsection">
            <span class="subsection-title">3. MCSE (Monte Carlo Standard Error)</span>
            <p><strong>What it is:</strong><br>
            MCSE measures "simulation noise." Because MCMC finds answers by random sampling rather than solving an exact equation, the result wobbles slightly if you run the code again. MCSE quantifies this computational error.</p>
            
            <p><strong>The Calculation:</strong></p>
            <div style="font-family: 'Times New Roman', serif; font-size: 1.1em; margin: 15px 0; display: flex; align-items: center; gap: 10px;">
                <span>MCSE = </span>
                <div style="display: inline-block; vertical-align: middle; text-align: center;">
                    <div style="border-bottom: 1px solid currentColor; padding-bottom: 2px;">Posterior SD</div>
                    <div style="padding-top: 2px;">&radic;<span style="text-decoration: overline;">ESS</span></div>
                </div>
            </div>

            <p>It compares the simulation noise to the <strong>Posterior Standard Deviation (SD)</strong>. The Posterior SD represents the actual uncertainty in your result based on the evidence (the spread of plausible values).</p>

            <p><strong>How it assesses Quality:</strong><br>
            It determines if your estimate is stable enough to trust.</p>

            <p><strong>The Logic:</strong> Higher ESS (more info) reduces MCSE (less wobble).</p>

            <p><strong>The Quality Check:</strong> You want the MCSE to be tiny compared to the Posterior SD. This ensures the uncertainty you report comes from your data, not just the computer's random variation.</p>
        </div>
        <div class="subsection">
            <span class="subsection-title">MCMC Workflow Summary</span>
            <p>1. ÀÜR (Gelman-Rubin statistic)<br>
            Confirms chains have mixed (validity). Check if ÀÜR ‚âà 1 (ÀÜR ‚â§ 1.01 is safe).</p>
            <p>2. ESS (Effective Sample Size)<br>
            Measures the volume of independent information (quantity). Aim for ESS >400 or >1000 per chain for stable estimates.</p>
            <p>3. MCSE (Monte Carlo Standard Error)<br>
            Uses ESS to assess if your estimate is precise enough for your scientific question (quality).</p>
        </div>
        <div class="subsection">
            <span class="subsection-title">Trace Plot: Monitoring Convergence at a glance</span>
            <p>"Fuzzy caterpillar" is a great rule of thumb, but not fail-safe. It implies mixing, but watch for:<br>
            1. Sticking: Subtle "flat shelves" mean the sampler froze.<br>
            2. Local Traps: A chain can look stable while stuck in one peak, missing others.</p>
            <p>Treat visuals as a sniff test. Always verify with R^ (to check consensus) and ESS (to check volume) to ensure the fuzz is real.</p>
        </div>
    </div>
</details>

            <!-- SECTION 7 -->
            <details class="section">
                <summary>3. Hamiltonian Monte Carlo (HMC)</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">How HMC Works Step by Step</span>
                        <p>HMC treats the posterior distribution like a physical terrain to explore it efficiently. Here is how the machinery works, step-by-step:</p>
                        <p><strong>1. The Landscape (Negative Log-Probability)</strong><br>
                        To apply physics to statistics, we treat the probability distribution as a "Potential Energy" field.<br>
                        ‚Ä¢ The Inversion: In physics, gravity pulls objects to the lowest energy state (the bottom of a valley). In statistics, we want the highest probability (the top of a peak). By taking the negative logarithm of the probability, we flip the map upside down: high-probability peaks become deep valleys that the sampler naturally "falls" into.<br>
                        
                        <p><strong>2. Momentum and The Mass Matrix</strong><br>
                        To move around this landscape, we give the sampler a physical "kick." We pair every parameter with an auxiliary momentum variable.<br>
                        Think of the mass matrix as defining the "weight" of the ball, but with a twist: you can make the ball heavy in one direction and light in another. The Mass Matrix creates the shape of the momentum distribution. </p>
                        <p>Selecting a mass matrix is how you shape the distribution of the momentum variable. Hence, bad mass matrix -> struggle to explore the landscape because the ball may slow down or speed up ineffectively.</p>
                        <p>In practice, HMC spends a "warm-up" phase learning the shape of the valley to select the appropriate matrix.<br></p>
                        
                        <p><strong>3. Simulation</strong><br>
                        The sampler trades Potential Energy for Kinetic Energy: it speeds up as it dives into high-probability valleys and slows down as it climbs low-probability hills. This allows it to explore the entire shape of the "bowl" (the posterior) rather than just sitting at the bottom.</p>
                        <p><strong>4. Acceptance / Rejection of Samples through Energy Conservation</strong><br>
                        In a perfect physical system, Total Energy (Potential + Kinetic) is conserved. However, because computer simulations use discrete time steps, tiny errors creep in. We compare the energy at the start and end of the trajectory. If the energy is roughly conserved, the move is accepted. This correction step ensures the resulting samples are statistically valid.</p>
                        <p><strong>5. Divergences (The "Check Engine" Light)</strong><br>
                        If the landscape is too treacherous‚Äîlike a "funnel" with a dangerously steep neck‚Äîthe simulated physics break, and energy shoots to infinity.<br>
                        These divergent transitions are invaluable diagnostics. They usually cluster in specific problem areas, pinpointing exactly where the geometry is broken. </p>
                        <p>The geometry is the crash site; the industry context explains why the road was built that way.</p>
                        <p>This guides reparameterization: the process of mathematically rewriting variables to reshape the terrain without changing predictions. </p>
                    </div>
                   
                </div>
            </details>

            <!-- SECTION 8 -->
            <details class="section">
                <summary>4. Comparing Models</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">P_Loo (Effective Parameters via LOO)</span>
                        <p><strong>Leave One Out Mechanics Reminder</strong><br>
                        LOO is just a procedure. It produces no numbers or results on its own without a metric (like ELPD, MSE, or Accuracy) to calculate during that procedure.<br>
                        You cannot "just LOO"‚Äîyou must LOO with a scorecard.</p>
                        <p><strong>STEPS</strong><br>
                        1. Hide y_i (leave it out).<br>
                        2. Train the model on the remaining n-1 points.<br>
                        3. Score the log probability of the hidden y_i.<br></p>
            
                    </div>
                    <div class="subsection">
                        <span class="subsection-title"><strong>Pareto Shape Parameter</strong></span>
                        <p> IDENTIFYING OVER-EMPHASIZED DATAPOINTS <br>
                        <p>Estimates how much the model relies on a single data point.</p>
                        <p>- Low Pareto Values<br>
                        The data point is normal; the model understands it easily alongside the others.</p>
                        <p>- High Pareto Values (> 0.7)<br>
                        The data point is highly influential and "surprising." The model struggles to fit it and is likely overfitting. Removing this point would drastically change predictions. It also signals that the posterior is too heavy-tailed for reliable approximation.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">p_loo: Effective Number of Parameters</span>
                        <p>p‚Çó‚Çí‚Çí measures the gap between in-sample fit (memorization) and out-of-sample generalization.</p>
                        <p><strong>Formula</strong><br>
                        p‚Çó‚Çí‚Çí = lppd ‚àí elpd‚Çó‚Çí‚Çí<br>
                        - lppd: LPPD is a score of how well the model's parameters generate the historical data.<br>
                        - elpd‚Çó‚Çí‚Çí: ELPD is a score of how well the model generates new data.</p>
                        <p><strong>Interpretation</strong><br>
                        p‚Çó‚Çí‚Çí represents the model's overfitting tendency.<br>
                        - Ideal: p‚Çó‚Çí‚Çí ‚âà actual number of parameters (e.g., ~5 for 5 variables).<br>
                        - Very large p‚Çó‚Çí‚Çí: Model is overly flexible and overfitting noise heavily.<br>
                        - p‚Çó‚Çí‚Çí >> actual parameters: Model relies too much on influential data points (high leverage) or the prior is too weak.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

    <!-- PART III -->
    <details class="part">
        <summary>Part III: Advanced Models</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Lasso, Statespace, Kalman Filter</span>

            <!-- SECTION 9 -->
            <details class="section">
                <summary>1. Laplace Priors and Bayesian Lasso</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">The "Skeptical" Distribution</span>
                        <p>"Irrelevant until proven Essential."</p>
                        <p><strong>How it works as a test:</strong> When you aren't sure if a variable (like a specific sensor, a gene, or a market indicator) is predictive, you assign it a Laplace Prior.<br>
                        1. The Prior pulls every single variable toward zero (meaning: "this variable has no effect").<br>
                        2. The Data fights back. If a variable effectively lowers the error rate, the Data pulls it away from zero.<br>
                        3. The Result: Because the Laplace peak is so sharp, weak variables slide all the way down to exactly zero and "die." Only the variables with genuine, strong predictive power can survive the pull and remain non-zero.</p>
                        <p><strong>When to use it:</strong><br>
                        ‚Ä¢ High-Dimensional "Fishing": You have 1,000 potential variables but suspect only 5 actually matter.<br>
                        ‚Ä¢ Expensive Data: When keeping a variable "active" costs money (e.g., requires keeping a sensor online), you want to zero out the useless ones.<br>
                        ‚Ä¢ Interpretability: You need to explain to a human why the prediction happened (it's easier to point to 3 active variables than 1,000 slightly active ones).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Sector Examples</span>
                        <p><strong> Defense: Acoustic Signature Classification (Sonar)</strong><br>
                        The Unknown: A submarine's passive sonar picks up a complex noise in the water. This sound is a mix of thousands of frequencies. You need to know if this is a whale, a merchant ship, or an enemy submarine. Most frequencies are just "ocean background noise," but you don't know which frequencies contain the hidden mechanical hum of a propeller.<br>
                        The Application: You treat every frequency band (Hz) as a variable. You apply a Laplace Prior to the amplitude weights of these bands.<br>
                        The Result: The Laplace Prior aggressively suppresses the thousands of frequencies related to waves and shrimp (background noise), forcing their weights to zero. It leaves behind a "sparse" set of specific non-zero frequencies‚Äîperhaps a harmonic series at 50Hz, 100Hz, and 150Hz.<br>
                        ‚Ä¢ Outcome: This sparse "fingerprint" is isolated from the noise. The system instantly recognizes this specific harmonic pattern as the signature of a specific class of diesel-electric engine, ignoring the rest of the ocean's chaotic noise.</p>
                        
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Hamiltonian Monte Carlo + LaPlace Priors = Bayesian Lasso</span>
                        <p>HMC uses gradients to navigate parameter space like a skateboarder in a half-pipe. A Laplace Prior modifies this pipe, carving a deep, narrow trench at zero.</p>
                        <p>[[As HMC samples, it naturally slides the coefficients of irrelevant variables into this trench, trapping their probability mass near zero.<br>
                        However, essential features possess enough "data energy" (evidence) to push the skateboarder up the walls, keeping their values away from zero.]]</p>
                        <p>This combination (often called the Bayesian Lasso) is powerful because HMC explores high-dimensional spaces efficiently. It doesn't just select features; it quantifies certainty. You get a histogram for every variable: if the samples pile up at zero, it‚Äôs noise; if they hover strictly away from zero, the feature is essential.</p>
                    </div>
                </div>
            </details>

         <!-- SECTION 10 -->
<details class="section">
    <summary>2. Statespace Modeling</summary>
    <div class="section-content">
        <div class="subsection">
            <span class="subsection-title">State Space Modeling</span>
            <p>Track a system that changes over time, where you cannot measure the variables you actually care about.</p>
                               
        
            <p><strong>Latent State:</strong> Information not directly observable, but that can be approximated using transformed data.</p>
        </div>
        <div class="subsection">
            <span class="subsection-title">
                1. FILTERING
                
                <!-- BUTTON START -->
                <button class="eye-btn" onclick="openViewport('smooth_filter.html', 'FILTERING DYNAMICS')" title="Visualize Filtering">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                    </svg>
                </button>
                <!-- BUTTON END -->
            </span>
            <p>It is a rhythm of Prediction and Correction.<br></p>
            <p>Filtering is the process of estimating the true state (ùêó‚Çú) by balancing a past prediction (ùêó‚Çú‚Çã‚ÇÅ) against a measurement (ùê≤).</p>
            
           
            
            
            <p><strong>When to the marginal distribution:</strong><br>
            You use this whenever you need to estimate a variable that is dynamic (changing) and indirectly observed (noisy).<br>
            Robotics/GPS: Your phone uses this (often a Kalman Filter). The GPS sensor is accurate to ~10m, but your blue dot on maps moves smoothly. That smooth movement is the hidden state ùê± being filtered from the noisy GPS data ùê≤.<br>
            </p>
        </div>
        <div class="subsection">
            <span class="subsection-title">
                2. SMOOTHING

                <!-- BUTTON START -->
                <button class="eye-btn" onclick="openViewport('smooth_filter.html', 'SMOOTHING DYNAMICS')" title="Visualize Smoothing">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        <rect x="2" y="10" width="1" height="1" /><rect x="21" y="13" width="1" height="1" />
                    </svg>
                </button>
                <!-- BUTTON END -->
            </span>
            <p>You aren't predicting tomorrow; you are refining the history.<br>
            It computes the probability of a past state ùê±‚Çñ using the entire dataset ùê≤‚ÇÄ:‚Çú‚Äîmeaning it uses observations from before, during, and after the specific moment you are analyzing.</p>
            
            
            
            
            <p><strong>When to use it?</strong><br>
            You use Smoothing for retrospective analysis or offline processing, where you have recorded data and need the highest accuracy possible, not real-time responsiveness.<br>
            * Climate Science: Reconstructing past temperatures (ùê±) using ice cores and tree rings (ùê≤). You aren't predicting tomorrow; you are refining the history of the last 100 years.<br></p>
            
        </div>
    </div>
</details>

            <!-- SECTION 11 -->
            <details class="section">
                <summary>3. Linear Gaussian SSM</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Explained through a missile navigation system</span>
                        <p><strong>1. The State Evolution Equation</strong><br>
                        <span class="code-block">ùêó‚Çú = ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ + ùõà‚Çú</span><br>
                        This equation defines the "physics" of the hidden system.<br>
                        * ùêó‚Çú (State Vector): A column of numbers representing the complete internal status of the system. For a moving object, this vector might contain [position, velocity, acceleration]. It holds all dimensions required to predict the future, even those not directly visible.<br>
                        * ùêÖ‚Çú (Transition Matrix): It encodes the rules of interaction between state variables. It is a linear operator that multiplies the previous state vector (ùêó‚Çú‚Çã‚ÇÅ) to calculate the new state. For example, it mathematically enforces that "current position = previous position + previous velocity."<br>
                        * ùõà‚Çú (Process Noise Vector): A number column of random shocks representing external realities not captured by your physics engine (e.g., wind gusts or wheel slip).<br>
                        * êêê‚Çú (Process Covariance Matrix): The numbers on the diagonal tell you the magnitude (variance) of the shocks for each variable. The off-diagonal numbers tell you the correlations‚Äîif the velocity gets a random shock, does the position also tend to get a shock? </p>
                        <p><strong>2. The Observation Equation</strong><br>
                        <span class="code-block">ùêò‚Çú = ùêá‚Çú ùêó‚Çú + ùõÜ‚Çú</span><br>
                        This equation defines the "lens" through which we view the system.<br>
                        * ùêò‚Çú (Observation Vector): The raw data collected by your sensors at time ùë°. This vector typically has fewer dimensions than the state vector (e.g., a GPS sensor reads [position], but cannot read [velocity]).<br>
                        * ùêá‚Çú (Observation Matrix):  It acts as a projection operator, mapping the high-dimensional "State Space" down to the lower-dimensional "Measurement Space."  This matrix describes the relationship between the hidden truth and the sensor reading.<br>
                        * ùõÜ‚Çú (Measurement Noise Vector): A vector representing the random error or "static" inherent in the sensors.<br>
                        * ùêë‚Çú (Measurement Covariance Matrix): This matrix quantifies the reliability of your sensors. A high value on the diagonal means that specific sensor is very noisy (high variance); a low value means it is precise. The Kalman Filter uses this matrix to decide how much to "trust" the incoming data ùêò‚Çú versus its own prediction from Step 1.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Operational Interactions: The Mechanics of the Matrix</span>
                        <p> ùêÖ‚Çú * ùêó‚Çú‚Çã‚ÇÅ</p>
                        <p><strong>1. The State Update: Linear Combination and Displacement</strong><br>
                        [[[ ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ + ùõà‚Çú<br>
                        The product of the transition matrix [ùêÖ‚Çú] and the previous values of the state vector [ùêó‚Çú‚Çã‚ÇÅ] is used to ensure the transformation rules are being enforced. ]]]<br>
                        ‚ñ¨ŒπìÜÉ<br>
                                              
                        
                        <p><strong>2. The Observation Projection: Translation and Selection</strong><br>
                        [[[ ùêá‚Çú ùêó‚Çú<br>
                        The product of the obs matrix [ùêá‚Çú ] by the state vector [ùêó‚Çú]is to filter out latent variables. ]]]<br>
                        By performing the multiplication, you are essentially saying:<br>
                        "Take the reality of the past (ùêó‚Çú‚Çã‚ÇÅ) and force it through the logic of my physics engine (ùêÖ‚Çú) to calculate where it must be now."<br>
                        ‚ñ¨ŒπìÜÉ<br>
                        
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Generative Process of the Linear Gaussian State Space Model (SSM)</span>
                        <p>The generative process asks, "If the system works like this, what kind of data would it produce?"</p>
                        
                        <p><strong>1. Equation 1: The Physics (Process Model)</strong><br>
                        <span class="code-block">ùêó‚Çú ‚àº ùí©(ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ, ùêê‚Çú)</span><br>
                        This equation says: "The State at time ùë° is a random draw from a Bell Curve."</p>
                        <p>A. Why is the Mean ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ?<br>
                        In a Normal distribution, the Mean represents the "perfect," "expected," or "most likely" value.<br>
                        The term ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ is your deterministic physics engine. It applies the transformation rules (e.g., position + velocity) to the previous state.<br>
                        If we lived in a perfect universe with no wind, friction, or entropy, the state would be exactly ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ.<br>
                        Therefore, we center the bell curve right on top of this physical prediction. It is our "best guess."</p>
                        <p>B. Why is ùêê‚Çú the Sigma (Covariance)?<br>
                        
                        This represents the "Process Noise." It accounts for external realities the physics engine ignored (wind gusts, wheel slips).</p><br>
                        <p><strong>2. Equation 2: The Sensor (Observation Model)</strong><br>
                        <span class="code-block">ùêò‚Çú ‚àº ùí©(ùêá‚Çú ùêó‚Çú, ùêë‚Çú)</span><br>
                        This equation says: "The Data reading at time ùë° is a random draw from a Bell Curve centered around the truth."</p>
                        <p>A. Why is the Mean ùêá‚Çú ùêó‚Çú?<br>
                        ùêá‚Çú is the "lens" or "mask" matrix. It converts the hidden state (Truth) into what the sensor should theoretically see.<br>
                        If the sensor were perfect, the reading ùêò‚Çú would equal ùêá‚Çú ùêó‚Çú exactly.<br>
                        Therefore, we center the probability distribution on this "Ideal Reading."</p>
                        <p>B. Why is ùêë‚Çú the Sigma (Covariance)?<br>
                        ùêë‚Çú represents Measurement Noise.<br>
                        This defines the quality of your hardware. A cheap sensor has a high ùêë (wide spread, fuzzy data). An expensive, precision sensor has a low ùêë (narrow spread, sharp data).</p>
                        <p><strong>3. Summary of Interactions</strong><br>
                        Kalman Filter mathematically weighs two competing uncertainties:<br>
                        The Physics Uncertainty [unobservable state] (ùêê‚Çú): "How much do I trust my prediction?"<br>
                        The Sensor Uncertainty [observable data] (ùêë‚Çú): "How much do I trust my data?"<br>
                        If ùêê is large and ùêë is small: The math "trusts" the data. The bell curve is wide for the prediction but narrow for the measurement, so the final estimate snaps to the measurement.<br>
                        If ùêê is small and ùêë is large: The math "trusts" the physics. The data is too noisy, so the filter ignores the data and sticks to the trajectory calculated by ùêÖ‚Çú ùêó‚Çú‚Çã‚ÇÅ.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 12 -->
            <details class="section">
                <summary>4. Kalman Filter</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">The Mechanics</span>
                        <p>Think of the Kalman Gain as a dynamic mixing slider that moves automatically at every time step between 0 (Pure Physics) and 1 (Pure Data).</p>
                        <p><strong>1. The Mechanics of the Gain</strong><br>
                        The Gain compares the Predicted Uncertainty (derived from ùêê) against the Measurement Uncertainty (ùêë).<br>
                        <span class="code-block">ùêä‚Çú ‚âà Uncertainty_Predict / (Uncertainty_Predict + Uncertainty_Measure)</span></p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Scenarios</span>
                        <p><strong>2. Scenario A: High ùêê, Low ùêë (Trust the Data)</strong><br>
                        * The Situation: You are tracking an erratic drone (High Process Noise) with a laser precision tracker (Low Measurement Noise).<br>
                        * The Cycle: During the Predict step, the drone's erratic behavior makes the uncertainty "bubble" expand rapidly.<br>
                        * The Action: The math sees the Prediction is vague but the Sensor is sharp. The Kalman Gain shoots up (close to 1).<br>
                        * The Result: In the Update step, the filter aggressively snaps the state to the measurement, effectively resetting the prediction to match the laser.</p>
                        <p><strong>3. Scenario B: Low ùêê, High ùêë (Trust the Physics)</strong><br>
                        * The Situation: You are tracking a heavy freight train (Low Process Noise) with a fuzzy analog camera (High Measurement Noise).<br>
                        * The Cycle: During the Predict step, the train's massive inertia means the uncertainty stays tight‚Äîwe know exactly where it should be.<br>
                        * The Action: The math sees the Sensor is a "shotgun blast" of noise compared to the tight Physics prediction. The Kalman Gain drops (close to 0).<br>
                        * The Result: In the Update step, the filter treats the observation as "static." It barely nudges the state, allowing the smooth physics trajectory to dominate.</p>
                        <p><strong>Summary</strong><br>
                        The Kalman Filter aids the cycle by optimizing the correction. It prevents the system from chasing "ghosts" (sensor noise) while ensuring it doesn't become "blind" to real changes in direction.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

</div>

<!-- VISUALIZATION VIEWPORT CONTAINER -->
<div id="viz-viewport" class="retro-viewport dither-bg">
    <div class="vp-header">
        <span id="vp-title">VISUALIZATION</span>
        <button class="vp-close" onclick="closeViewport()">[CLOSE_X]</button>
    </div>
    <div class="vp-body">
        <iframe id="vp-iframe" src=""></iframe>
    </div>
</div>

<script>
    const viewport = document.getElementById('viz-viewport');
    const iframe = document.getElementById('vp-iframe');
    const title = document.getElementById('vp-title');

    function openViewport(url, label) {
        iframe.src = url;
        title.innerText = label || "SYSTEM_VISUALIZER";
        viewport.classList.add('active');
    }

    function closeViewport() {
        viewport.classList.remove('active');
        // Clear src to stop animations/audio when closed
        setTimeout(() => { iframe.src = ""; }, 200);
    }
</script>

</body>
</html>