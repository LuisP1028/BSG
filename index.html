<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Bayesian Modeling & Computation: Study Guide</title>
<style>
    /* 
       --- EDITABLE CHUNK START --- 
       Modify these variables to change the color scheme.
    */
    :root {
        --bg-color: #000000;           /* Pure Black Background */
        --text-color: #00ff41;         /* Terminal Green Text */
        --accent-color: #00ff41;       /* Bright Green Accents */
        --dim-color: #003b00;          /* Dark Green for subtle borders */
        --border-color: #00ff41;       /* Green Borders */
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden; /* Prevents horizontal scroll on mobile */
    }

    /* --- DITHERPUNK VISUALS (Green Mode) --- */
    
    /* Background Dither Simulation */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        /* Updated to Green dither dots */
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    /* Scanline Overlay */
    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        /* Green-tinted scanlines */
        background: linear-gradient(
            to bottom, 
            rgba(0, 255, 65, 0), 
            rgba(0, 255, 65, 0) 50%, 
            rgba(0, 20, 0, 0.2) 50%, 
            rgba(0, 20, 0, 0.2)
        );
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    /* --- RESPONSIVE CONTAINER --- */
    .container {
        max-width: 900px;         /* Limits width on large screens */
        width: 100%;              /* Ensures it takes full width on small screens */
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9); /* Very dark green background tint */
        min-height: 100vh;
    }

    /* Typography */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 3rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        letter-spacing: -2px;
        text-shadow: 0px 0px 8px var(--accent-color); /* Green Glow */
        color: var(--accent-color);
        text-align: center;
        word-wrap: break-word; /* Prevents long words breaking layout */
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }

    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION SYSTEM --- */

    /* PART LEVEL (Outer) */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    
    details.part[open] {
        box-shadow: 4px 4px 0px var(--dim-color);
        transform: translate(2px, 2px);
    }

    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color); /* Black text on Green background */
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }

    details.part > summary::-webkit-details-marker { display: none; }
    
    details.part > summary::after {
        content: '+'; 
        position: absolute; right: 20px; font-weight: 900;
    }
    details.part[open] > summary::after { content: '-'; }

    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }
    
    .focus-line {
        display: block;
        font-size: 0.8rem;
        text-transform: uppercase;
        letter-spacing: 2px;
        margin-bottom: 20px;
        color: #008f11;
        font-weight: bold;
    }

    /* SECTION LEVEL (Middle) */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }

    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }

    details.section > summary:hover { 
        background: var(--dim-color); 
        color: var(--accent-color); 
    }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }

    .section-content { padding: 20px; }

    /* SUBSECTION / CONTENT BLOCKS */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }

    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    /* Code/Math styling */
    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto; /* Ensures horizontal scrolling on small screens */
        white-space: pre-wrap; /* Wraps text if possible */
    }

    /* --- RESPONSIVE ADJUSTMENTS --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; } /* Justify can look weird on mobile */
    }
    /* --- END EDITABLE CHUNK --- */
</style>
</head>
<body>

<div class="dither-layer"></div>
<div class="scanlines"></div>

<div class="container">
    <h1>Bayesian Modeling<br>& Computation</h1>

    <!-- PART I -->
    <details class="part">
        <summary>Part I: Bayesian Foundations</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Concepts, MCMC, Priors</span>

            <!-- SECTION 1 -->
            <details class="section">
                <summary>1. Key Bayesian Concepts</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Prior</span>
                        <p>The prior distribution represents initial beliefs about model parameters before observing any data. It encodes pre-existing knowledge or assumptions about parameter values.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Observed data</span>
                        <p>Observed data are the actual measurements, used to update prior beliefs via the likelihood function in Bayesian inference.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Prior predictive distribution</span>
                        <p>The distribution of data that the model would generate based solely on the prior beliefs about parameters, before observing any actual data.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Marginal likelihood</span>
                        <p>The probability of the observed data under the model.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Computing expectations</span>
                        <p>The average outcome of a quantity (like a parameter or prediction) under the posterior or predictive distribution.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 2 -->
            <details class="section">
                <summary>2. MCMC & Metropolis Hastings</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Markov Chain Monte Carlo (MCMC)</span>
                        <p> MCMC approximates parameter specifications, it builds a chain of samples that maps the posterior distribution's shape and uncertainty
                            .</p>
                        <p>It builds a Markov chain: starting from an initial point, it repeatedly proposes new "guesses" from a proposal distribution and decides whether to accept them, eventually producing samples that follow the target posterior.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Metropolis Hastings [ACCEPTING/REJECTING SAMPLES]</span>
                        <p><strong>1. The Current Stance:</strong> The algorithm is currently standing at a specific parameter value (let's call it Point A). It knows the "elevation" of this point by calculating the posterior density (Likelihood x Prior).</p>
                        <p><strong>2. The Proposal (The "What if?"):</strong> The algorithm blindly suggests a new spot nearby, usually by picking a random coordinate close to Point A. Let's call this Point B.</p>
                        <p><strong>3. The Comparison:</strong> It compares the elevation of Point B to Point A.</p>
                        <ul>
                            <li><strong>Case 1: Point B is higher (More Probable).</strong> If the posterior density at B is higher than at A, the move is a no-brainer. The algorithm Accepts immediately and moves to Point B.</li>
                            <li><strong>Case 2: Point B is lower (Less Probable).</strong> This is the genius part. It doesn't automatically reject the move. Instead, it calculates the ratio of the heights. If Point B is 70% as high as Point A, the algorithm rolls a virtual 10-sided die.
                                <ul>
                                    <li>If the roll is 7 or lower (70% chance), it Accepts and moves to the lower ground anyway.</li>
                                    <li>If the roll is 8, 9, or 10, it Rejects the move and stays at Point A.</li>
                                </ul>
                            </li>
                        </ul>
                        <p><strong>4. Recording the Sample:</strong></p>
                        <ul>
                            <li>If it Accepted: Point B becomes the new current position, and Point B is added to the list of samples.</li>
                            <li>If it Rejected: The hiker stays at Point A, and Point A is added to the list of samples again.</li>
                        </ul>
                        <p><strong>Why accept "worse" spots?</strong> If the hiker only ever moved uphill, they would get stuck on the top of the first small hill they found (a local maximum) and never discover the Mount Everest (global maximum) across the valley. By occasionally accepting downhill moves, the algorithm ensures it can traverse the valleys to find the true peaks of the posterior distribution.</p>
                    </div>
                </div>
            </details>



            <!-- SECTION 4 -->
            <details class="section">
                <summary>3. Types of Priors</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Conjugate Priors</span>
                        <p>"Same family in, same family out."</p>
                        <p>â€œA prior is conjugate to a likelihood if the posterior belongs to the same family of distributions as the prior.â€</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Objective Priors</span>
                        <p>â€œIf you do not have information about a problem, then you do not have any reason to believe one outcome is more likely than any other.â€</p>
                        <p><strong>Reparametrization:</strong> Describing the exact same system using different variables.<br>
                        </p>
                        <p><strong>JEFFREYâ€™S PRIOR:</strong> Identifies sensitive parameter values. The "Gold Standard" for 1D problems (estimating one thing).<br>
                        Invariant under reparametrization.<br>
                        
                        <p><strong>REFERENCE PRIORS:</strong> Maximize Kullback-Leibler Divergence; measures distance between posterior and prior distributions, with more distance being better [higher information distance]. By not encoding any bias on the prior, this lets the data speak as we collect more samples.<br>
                        KL Divergence measures the "information distance" between two distributions, P(x) (posterior) and Q(x) (prior)</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Maximum Entropy (MaxEnt) Priors</span>
                        <p>When we aren't totally clueless about a parameter's plausible values, we can use Maximum Entropy (MaxEnt) priors. By selecting the flattest prior possible that obeys your constraints, you maximize you admitted uncertainty.</p>
                        <p>For example, in modeling interest rates, a half-normal prevents negative values. By maximizing entropy, we allow the data to speak for itself.</p>
                        <p><strong>LAGRANGIAN MULTIPLIERS</strong><br>
                        <p>The math says: the probability at any point is just an exponential decay of how much that point violates your constraint(s).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Prior Predictive Distribution</span>
                        <p>Itâ€™s a sanity check. You draw parameters from your prior, then simulate data from them. If this "fake" data looks impossibleâ€”like negative heights or million-degree daysâ€”you know your prior is unrealistic and needs tightening.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

    <!-- PART II -->
    <details class="part">
        <summary>Part II: Diagnostics & Workflow</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Analysis, Convergence, HMC, Comparison</span>

            <!-- SECTION 5 -->
            <details class="section">
                <summary>1. Exploratory Analysis</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Bayesian Test Statistic (T-Stat)</span>
                        
                        <p>In Bayesian posterior predictive checks (PPCs) the "Test Statistic" refers to any metric you choose to stress-test your model.</p>
                        <p>You define the test statistic based on what you suspect the model is missing.<br>
                        
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Bayesian P-Values</span>
                        
                        <p>p_B is simply the proportion of simulations where T_sim > T_obs.</p>
                        <p>An ideal p_B is â‰ˆ0.5, meaning the real data sits comfortably in the middle of the model's predictions. Values near 0 or 1 indicate systematic bias.</p>
                        <p>The Bayesian P-value is the probability that the simulated statistic is greater than the observed statistic.</p>
                        
                        <hr style="opacity: 0.3; margin: 10px 0;">
                        
                        <p><strong>Calculation Process:</strong></p>
                        <ol style="margin-top: 5px; padding-left: 20px;">
                            <li>
                                <strong>Calculate Observed T:</strong> Calculate the test statistic <em>T</em> once for the real observed data: <em>T(y_obs)</em><br>
                                <span style="opacity: 0.8; font-size: 0.9em;">(e.g., the standard deviation of the real data is 15.2).</span>
                            </li>
                            <li style="margin-top: 8px;">
                                <strong>Simulate Data:</strong> Generate 1,000 simulated datasets (<em>y_rep</em>) from the model.
                            </li>
                            <li style="margin-top: 8px;">
                                <strong>Calculate Simulated T:</strong> Calculate the same test statistic <em>T</em> for each of the 1,000 simulated datasets: <em>T(y_rep)</em><br>
                                <span style="opacity: 0.8; font-size: 0.9em;">(e.g., 14.1, 15.9, 13.8, 16.0, etc.).</span>
                            </li>
                            <li style="margin-top: 8px;">
                                <strong>Compute Proportion:</strong> The Bayesian p-value (<em>p_B</em>) is the proportion (or percentage) of the 1,000 simulated <em>T(y_rep)</em> values that are greater than the observed <em>T(y_obs)</em>.
                            </li>
                        </ol>
                    </div>
                </div>
            </details>
        </div>
    </details>
            <!-- SECTION 6 -->
            <details class="section">
                <summary>2. Convergence: R-Hat, ESS, MCSE</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">1. R Hat: Convergence Factor [Gelman-Rubin]</span>
                        <p>MCMC methods have theoretical guarantees of correctness regardless of starting point, but only for infinite samples. In practice, we need ways to assess convergence for finite samples.</p>
                        <p>Ë†R compares the spread between different chains to the spread within them. If Ë†R > 1.01, your chains are effectively "disagreeing" or haven't mixed yet â€” the simulation isn't finished, so don't trust the results.</p>
                        <p><strong>2. ESS (Effective Sample Size)</strong><br>
                        Check this second to measure precision.<br>
                        Once chains have converged (Ë†R â‰ˆ 1), ESS tells you the volume of useful information in your samples.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">2. EFFECTIVE SAMPLE SIZE (ESS)</span>
                        <p>You use MCMC samples' statistical properties to find the ESS.</p>
                        <p><strong>The Calculation Steps</strong><br>
                        Since the true autocorrelations Ï_k are unknown, they are estimated from the samples themselves:</p>
                        <p>1. Generate Samples<br>
                        Run your MCMC algorithm</p>
                        <p>2. Calculate Autocorrelation<br>
                        Compute the empirical autocorrelation ÏÌ‚_k for different lags k = 1, 2, 3, </p>
                        <p>3. Sum the Correlations (Truncation)<br>
                        <p>Standard software (like Stan, ArviZ, or R's coda) truncates the sum when the autocorrelation drops below a certain threshold. This step is crucial to filter out noise.</p>
                        <p>4. Final Division<br>
                        Divide the total number of samples N by this summed quantity (Ï„) to obtain the effective sample size.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">3. MCSE (Monte Carlo Standard Error)</span>
                        <p>MCSE is the "simulation noise"â€”the error introduced by using an approximate algorithm instead of exact math.</p>
                        <p>Itâ€™s calculated as:<br>
                        MCSE = SD / âˆšESS<br>
                        Where SD is the posterior standard deviation and ESS is the effective sample size.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">MCMC Workflow Summary</span>
                        <p>1. Ë†R (Gelman-Rubin statistic)<br>
                        Confirms chains have mixed (validity). Check if Ë†R â‰ˆ 1 (Ë†R â‰¤ 1.01 is safe).</p>
                        <p>2. ESS (Effective Sample Size)<br>
                        Measures the volume of independent information (quantity). Aim for ESS >400 or >1000 per chain for stable estimates.</p>
                        <p>3. MCSE (Monte Carlo Standard Error)<br>
                        Uses ESS to assess if your estimate is precise enough for your scientific question (quality).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Trace Plot: Monitoring Convergence at a glance</span>
                        <p>"Fuzzy caterpillar" is a great rule of thumb, but not fail-safe. It implies mixing, but watch for:<br>
                        1. Sticking: Subtle "flat shelves" mean the sampler froze.<br>
                        2. Local Traps: A chain can look stable while stuck in one peak, missing others.</p>
                        <p>Treat visuals as a sniff test. Always verify with R^ (to check consensus) and ESS (to check volume) to ensure the fuzz is real.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 7 -->
            <details class="section">
                <summary>3. Hamiltonian Monte Carlo (HMC)</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">How HMC Works Step by Step</span>
                        <p>HMC treats the posterior distribution like a physical terrain to explore it efficiently. Here is how the machinery works, step-by-step:</p>
                        <p><strong>1. The Landscape (Negative Log-Probability)</strong><br>
                        To apply physics to statistics, we treat the probability distribution as a "Potential Energy" field.<br>
                        â€¢ The Inversion: In physics, gravity pulls objects to the lowest energy state (the bottom of a valley). In statistics, we want the highest probability (the top of a peak). By taking the negative of the probability, we flip the map upside down: high-probability peaks become deep valleys that the sampler naturally "falls" into.<br>
                        
                        <p><strong>2. Momentum (The Kick)</strong><br>
                        To move around this landscape, we give the sampler a physical "kick." We pair every parameter with an auxiliary momentum variable, sampled from a proposal distribution (typically a Gaussian).<br>
                        The shape of this distribution (the mass matrix) acts like a vehicle's suspension. If tuned correctly to the valley's geometry, it allows the sampler to traverse long distances and complex correlations efficiently, rather than stumbling blindly like standard random-walk samplers.</p>
                        <p>In practice, HMC spends a "warm-up" phase learning the shape of the valley to select the appropriate matrix:<br>
                        â€¢ Identity Matrix: The default setting. It assumes all parameters have the exact same scale and no correlations. It often struggles with real-world data (e.g., comparing "Income" to "Age").<br>
                        â€¢ Diagonal Matrix: The industry standard. It estimates the variance of each parameter independently. It effectively "squishes" long dimensions and stretches short ones to make the probability valley look like a round bowl.<br>
                        â€¢ Dense Matrix: The "heavy artillery" for highly correlated parameters. If the valley runs diagonally, a Dense matrix captures the covariance and rotates the entire coordinate system, turning a tight, diagonal ravine into a perfectly aligned bowl.</p>
                        <p><strong>3. Simulation</strong><br>
                        The sampler trades Potential Energy for Kinetic Energy: it speeds up as it dives into high-probability valleys and slows down as it climbs low-probability hills. This allows it to explore the entire shape of the "bowl" (the posterior) rather than just sitting at the bottom.</p>
                        <p><strong>4. Acceptance (Energy Conservation)</strong><br>
                        In a perfect physical system, Total Energy (Potential + Kinetic) is conserved. However, because computer simulations use discrete time steps, tiny errors creep in. We compare the energy at the start and end of the trajectory. If the energy is roughly conserved, the move is accepted. This correction step ensures the resulting samples are statistically valid.</p>
                        <p><strong>5. Divergences (The "Check Engine" Light)</strong><br>
                        If the landscape is too treacherousâ€”like a "funnel" with a dangerously steep neckâ€”the simulation step size may fail to capture the curvature. When this happens, the physics break, and energy shoots to infinity.<br>
                        These divergent transitions are invaluable diagnostics. They usually cluster in specific problem areas, pinpointing exactly where the geometry is broken. This guides reparameterization: the process of mathematically rewriting variables to reshape the terrain without changing predictions. </p>
                    </div>
                   
                </div>
            </details>

            <!-- SECTION 8 -->
            <details class="section">
                <summary>4. Comparing Models</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">P_Loo (Effective Parameters via LOO)</span>
                        <p><strong>Leave One Out Mechanics Reminder</strong><br>
                        LOO is just a procedure (the act of rotating data). It produces no numbers or results on its own without a metric (like ELPD, MSE, or Accuracy) to calculate during that procedure.<br>
                        You cannot "just LOO"â€”you must LOO with a scorecard.</p>
                        <p><strong>STEPS</strong><br>
                        1. Hide y_i (leave it out).<br>
                        2. Train the model on the remaining n-1 points.<br>
                        3. Score the log probability of the hidden y_i.<br>
                        Repeat this n times (once for every point) and sum the scores.<br>
                        This removes the "double-dipping" bias, providing a realistic measure of how well the model generalizes to new data rather than memorizing the training set.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Pareto Shape Parameter</span>
                        <p><strong>IDENTIFYING OVER-EMPHASIZED DATAPOINTS</strong><br>
                        <p>In PSIS-LOO (Pareto Smoothed Importance Sampling Leave-One-Out), the shape parameter estimates how much the model relies on a single data point.</p>
                        <p>- Low Pareto Values<br>
                        The data point is normal; the model understands it easily alongside the others.</p>
                        <p>- High Pareto Values (> 0.7):<br>
                        The data point is highly influential and "surprising." The model struggles to fit it and is likely overfitting. Removing this point would drastically change predictions. It also signals that the posterior is too heavy-tailed for reliable approximation.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">p_loo: Effective Number of Parameters</span>
                        <p>pâ‚—â‚’â‚’ measures the gap between in-sample fit (memorization) and out-of-sample generalization.</p>
                        <p><strong>Formula</strong><br>
                        pâ‚—â‚’â‚’ = lppd âˆ’ elpdâ‚—â‚’â‚’<br>
                        - lppd: LPPD is a score of how well the model's parameters generate the historical data.<br>
                        - elpdâ‚—â‚’â‚’: ELPD is a score of how well the model generates new data.</p>
                        <p><strong>Interpretation</strong><br>
                        pâ‚—â‚’â‚’ represents the model's overfitting tendency.<br>
                        - Ideal: pâ‚—â‚’â‚’ â‰ˆ actual number of parameters (e.g., ~5 for 5 variables).<br>
                        - Very large pâ‚—â‚’â‚’: Model is overly flexible and overfitting noise heavily.<br>
                        - pâ‚—â‚’â‚’ >> actual parameters: Model relies too much on influential data points (high leverage) or the prior is too weak.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

    <!-- PART III -->
    <details class="part">
        <summary>Part III: Advanced Models</summary>
        <div class="part-content">
            <span class="focus-line">Focus: Lasso, Statespace, Kalman Filter</span>

            <!-- SECTION 9 -->
            <details class="section">
                <summary>1. Laplace Priors (Bayesian Lasso)</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">The "Skeptical" Distribution</span>
                        <p>"Irrelevant until proven Essential."</p>
                        <p><strong>How it works as a test:</strong> When you aren't sure if a variable (like a specific sensor, a gene, or a market indicator) is predictive, you assign it a Laplace Prior.<br>
                        1. The Prior pulls every single variable toward zero (meaning: "this variable has no effect").<br>
                        2. The Data fights back. If a variable effectively lowers the error rate, the Data pulls it away from zero.<br>
                        3. The Result: Because the Laplace peak is so sharp, weak variables slide all the way down to exactly zero and "die." Only the variables with genuine, strong predictive power can survive the pull and remain non-zero.</p>
                        <p><strong>When to use it:</strong><br>
                        â€¢ High-Dimensional "Fishing": You have 1,000 potential variables but suspect only 5 actually matter.<br>
                        â€¢ Expensive Data: When keeping a variable "active" costs money (e.g., requires keeping a sensor online), you want to zero out the useless ones.<br>
                        â€¢ Interpretability: You need to explain to a human why the prediction happened (it's easier to point to 3 active variables than 1,000 slightly active ones).</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Sector Examples</span>
                        <p><strong> Defense: Acoustic Signature Classification (Sonar)</strong><br>
                        The Unknown: A submarine's passive sonar picks up a complex noise in the water. This sound is a mix of thousands of frequencies. You need to know if this is a whale, a merchant ship, or an enemy submarine. Most frequencies are just "ocean background noise," but you don't know which frequencies contain the hidden mechanical hum of a propeller.<br>
                        The Application: You treat every frequency band (Hz) as a variable. You apply a Laplace Prior to the amplitude weights of these bands.<br>
                        The Result: The Laplace Prior aggressively suppresses the thousands of frequencies related to waves and shrimp (background noise), forcing their weights to zero. It leaves behind a "sparse" set of specific non-zero frequenciesâ€”perhaps a harmonic series at 50Hz, 100Hz, and 150Hz.<br>
                        â€¢ Outcome: This sparse "fingerprint" is isolated from the noise. The system instantly recognizes this specific harmonic pattern as the signature of a specific class of diesel-electric engine, ignoring the rest of the ocean's chaotic noise.</p>
                        
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Hamiltonian Monte Carlo + LaPlace Priors = Bayesian Lasso</span>
                        <p>HMC uses gradients to navigate parameter space like a skateboarder in a half-pipe. A Laplace Prior modifies this pipe, carving a deep, narrow trench at zero.</p>
                        <p>[[As HMC samples, it naturally slides the coefficients of irrelevant variables into this trench, trapping their probability mass near zero.<br>
                        However, essential features possess enough "data energy" (evidence) to push the skateboarder up the walls, keeping their values away from zero.]]</p>
                        <p>This combination (often called the Bayesian Lasso) is powerful because HMC explores high-dimensional spaces efficiently. It doesn't just select features; it quantifies certainty. You get a histogram for every variable: if the samples pile up at zero, itâ€™s noise; if they hover strictly away from zero, the feature is essential.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 10 -->
            <details class="section">
                <summary>2. Statespace Modeling</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">State Space Modeling</span>
                        <p>It is a mathematical framework used to track a system that changes over time, where you cannot measure the variables you actually care about.</p>
                        <p>Imagine a dynamic system where a latent state ğ±â‚œ evolves from ğ±â‚œâ‚‹â‚ via a transition model, strictly adhering to the Markov property.<br>
                        Since ğ±â‚œ is inaccessible, we rely on observations ğ²â‚œâ€”noisy emissions generated conditionally by the current state.</p>
                        <p>Effectively, it is the statistical reconstruction of the invisible trajectory of ğ± using only the imperfect measurements of ğ².</p>
                        <p><strong>Latent State:</strong> Information not directly observable, but that can be approximated using transformed data. We infer or estimate its probability distribution by conditioning on the history of observations ğ².</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">1. FILTERING</span>
                        <p>It is a rhythm of Prediction and Correction.<br>
                        It separates the world into two layers:<br>
                        The Hidden Layer (ğ±â‚œ): The "True State." This is reality. You never see this.<br>
                        The Observation Layer (ğ²â‚œ): The "Noisy Data." This is what your sensors tell you. (e.g., A blurry radar blip, a fluctuating stock price, or a thermometer reading with a margin of error).</p>
                       
                        <p><strong>Best illustrated with an example</strong><br>
                        Imagine you are driving into a tunnel and your GPS loses signal.<br>
                        Predict (The Physics): Even without signal, the GPS assumes you are still moving forward at 60mph. It uses the previous state (ğ±â‚œâ‚‹â‚) to predict the current state (ğ±â‚œ).<br>
                        Update (The Correction): Suddenly, you exit the tunnel and the satellite gets a ping (ğ²â‚œ). The ping says you are 10 meters back from where the model guessed.<br>
                        Filter: The model combines its guess with the new data. If the satellite signal is usually noisy/unreliable, it trusts the guess more. If the signal is precise, it trusts the data more. The result is the Filtered Estimate.</p>
                        <p><strong>What is the "Marginal Distribution"?</strong><br>
                        This answers the question: "Where are we right now?"<br>
                        <br>
                        <p>The marginal distribution aggregates all possible histories into a single snapshot of the present. While a "joint distribution" tracks the likelihood of every specific sequence of steps you took, the marginal distribution sums up the probabilities of every path that could have possibly led here. It doesn't "delete" the past; it collapses it. It consolidates the uncertainty of the entire journey to answer one specific question: "Given all the evidence so far, what is the probability I am at location X right now?".</p>
                        <p><strong>When to use it?</strong><br>
                        You use this whenever you need to estimate a variable that is dynamic (changing) and indirectly observed (noisy).<br>
                        Robotics/GPS: Your phone uses this (often a Kalman Filter). The GPS sensor is accurate to ~10m, but your blue dot on maps moves smoothly. That smooth movement is the hidden state ğ± being filtered from the noisy GPS data ğ².<br>
                        Finance: You observe stock prices (ğ²), but you want to trade based on "market volatility" (ğ±). Volatility isn't a number you can download; you must infer it from the price swings.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">2. SMOOTHING</span>
                        <p>You aren't predicting tomorrow; you are refining the history.<br>
                        Smoothing is the "hindsight" engine. While Filtering is about knowing where you are now based on the past, Smoothing is about figuring out where you were yesterday, knowing what you know today.<br>
                        It computes the probability of a past state ğ±â‚– using the entire dataset ğ²â‚€:â‚œâ€”meaning it uses observations from before, during, and after the specific moment you are analyzing.</p>
                        <p><strong>What does it do?</strong><br>
                        It refines the history. Because it has access to "future" data (relative to the state ğ±â‚– being estimated), it eliminates false moves and noise that the Filter fell for in real-time.<br>
                        If Filtering is "Live Reporting," Smoothing is the "Post-Game Analysis." It connects the dots to create the most probable coherent trajectory of the system after the event has concluded.</p>
                        <p><strong>How does it work?</strong><br>
                        It is typically a two-pass process, often called the Forward-Backward algorithm:<br>
                        Forward Pass (Filtering): You run the standard filter from time 0 to ğ‘‡. You make your best guesses in real-time.<br>
                        Backward Pass (Smoothing): You start at the end (time ğ‘‡) and walk backwards to time 0. You use the information from the end of the sequence to correct the estimates in the middle.</p>
                        
                        <p><strong>When to use it?</strong><br>
                        You use Smoothing for retrospective analysis or offline processing, where you have recorded data and need the highest accuracy possible, not real-time responsiveness.<br>
                        * Climate Science: Reconstructing past temperatures (ğ±) using ice cores and tree rings (ğ²). You aren't predicting tomorrow; you are refining the history of the last 100 years.<br>
                        * Economics: Dating the start and end of a recession. It is essentially impossible to know you are in a recession the day it starts (Filtering), but extremely easy to pinpoint the start date two years later (Smoothing).<br>
                        * Training Models: When teaching a computer to understand a dynamic system (Parameter Learning), you use smoothed estimates to train it on "what actually happened."</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 11 -->
            <details class="section">
                <summary>3. Linear Gaussian SSM</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">Explained through a missile navigation system</span>
                        <p><strong>1. The State Evolution Equation</strong><br>
                        <span class="code-block">ğ—â‚œ = ğ…â‚œ ğ—â‚œâ‚‹â‚ + ğ›ˆâ‚œ</span><br>
                        This equation defines the "physics" of the hidden system.<br>
                        * ğ—â‚œ (State Vector): A column of numbers representing the complete internal status of the system. For a moving object, this vector might contain [position, velocity, acceleration]. It holds all dimensions required to predict the future, even those not directly visible.<br>
                        * ğ…â‚œ (Transition Matrix): It encodes the rules of interaction between state variables. It is a linear operator that multiplies the previous state vector (ğ—â‚œâ‚‹â‚) to calculate the new state. For example, it mathematically enforces that "current position = previous position + previous velocity."<br>
                        * ğ›ˆâ‚œ (Process Noise Vector): A number column of random shocks representing external realities not captured by your physics engine (e.g., wind gusts or wheel slip).<br>
                        * ğâ‚œ (Process Covariance Matrix): The numbers on the diagonal tell you the magnitude (variance) of the shocks for each variable. The off-diagonal numbers tell you the correlationsâ€”if the velocity gets a random shock, does the position also tend to get a shock? </p>
                        <p><strong>2. The Observation Equation</strong><br>
                        <span class="code-block">ğ˜â‚œ = ğ‡â‚œ ğ—â‚œ + ğ›†â‚œ</span><br>
                        This equation defines the "lens" through which we view the system.<br>
                        * ğ˜â‚œ (Observation Vector): The raw data collected by your sensors at time ğ‘¡. This vector typically has fewer dimensions than the state vector (e.g., a GPS sensor reads [position], but cannot read [velocity]).<br>
                        * ğ‡â‚œ (Observation Matrix):  It acts as a projection operator, mapping the high-dimensional "State Space" down to the lower-dimensional "Measurement Space."  This matrix describes the relationship between the hidden truth and the sensor reading.<br>
                        * ğ›†â‚œ (Measurement Noise Vector): A vector representing the random error or "static" inherent in the sensors.<br>
                        * ğ‘â‚œ (Measurement Covariance Matrix): This matrix quantifies the reliability of your sensors. A high value on the diagonal means that specific sensor is very noisy (high variance); a low value means it is precise. The Kalman Filter uses this matrix to decide how much to "trust" the incoming data ğ˜â‚œ versus its own prediction from Step 1.</p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Operational Interactions: The Mechanics of the Matrix</span>
                        <p> Transition Matrix * t-1 State Vector</p>
                        <p><strong>1. The State Update: Linear Combination and Displacement</strong><br>
                        [[[ ğ…â‚œ ğ—â‚œâ‚‹â‚ + ğ›ˆâ‚œ<br>
                        The product of the transition matrix [ğ…â‚œ] and the previous values of the state vector [ğ—â‚œâ‚‹â‚] is used to ensure the transformation rules are being enforced. ]]]<br>
                        â–¬Î¹ğ“†ƒ<br>
                        â€¢ The Matrix-Vector Product (ğ…â‚œ ğ—â‚œâ‚‹â‚): This operation functions as a "mixing board." <br>
                        Consider a simple physics example where the state vector contains [Position, Velocity].<br>
                        The transition matrix [ğ…â‚œ ]contains the "rules"<br>
                        
                        
                        <p><strong>2. The Observation Projection: Translation and Selection</strong><br>
                        [[[ ğ‡â‚œ ğ—â‚œ<br>
                        The product of the obs matrix [ğ‡â‚œ ] by the state vector [ğ—â‚œ]is to filter out latent variables. ]]]<br>
                        By performing the multiplication, you are essentially saying:<br>
                        "Take the reality of the past (ğ—â‚œâ‚‹â‚) and force it through the logic of my physics engine (ğ…â‚œ) to calculate where it must be now."<br>
                        â–¬Î¹ğ“†ƒ<br>
                        * The State Vector (ğ—â‚œ) is the "Hidden Truth." It contains every detail the system tracks to function mathematically (e.g., [Position, Velocity, Acceleration]).<br>
                        * The Observation Matrix (ğ‡â‚œ) is the "Selector." It tells the equation which parts of that truth the sensor can actually see.</p>

                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Generative Process of the Linear Gaussian State Space Model (SSM)</span>
                        <p>The generative process asks, "If the system works like this, what kind of data would it produce?"</p>
                        <p><strong>1. The Notation</strong><br>
                        <br>
                        âˆ¼ (Tilde): "Is sampled from" or "follows the distribution of." When we say ğ— âˆ¼ ğ’©, we mean ğ— is a random variable generated by a Normal distribution.<br>
                        âˆ£ (Pipe): "Given." This indicates conditional probability. ğ‘(ğ—â‚œâˆ£ğ—â‚œâ‚‹â‚) asks: "What is the probability of the current state (ğ—â‚œ), given that we already know the previous state (ğ—â‚œâ‚‹â‚)?"<br>
                        â‰¡ (Triple Bar): "Is defined as" or "is equivalent to." This symbol links the abstract concept (the probability ğ‘) to the specific mathematical implementation (the Normal distribution ğ’©).<br>
                        ğ’©(ğ, Î£) (Script N): Represents the Normal (Gaussian) Distribution. It requires two arguments: the Mean (ğ, the center/peak) and the Covariance (Î£, the width/spread).</p>
                        <p><strong>2. Equation 1: The Physics (Process Model)</strong><br>
                        <span class="code-block">ğ—â‚œ âˆ¼ ğ’©(ğ…â‚œ ğ—â‚œâ‚‹â‚, ğâ‚œ)</span><br>
                        This equation says: "The State at time ğ‘¡ is a random draw from a Bell Curve."</p>
                        <p>A. Why is the Mean ğ…â‚œ ğ—â‚œâ‚‹â‚?<br>
                        In a Normal distribution, the Mean represents the "perfect," "expected," or "most likely" value.<br>
                        The term ğ…â‚œ ğ—â‚œâ‚‹â‚ is your deterministic physics engine. It applies the transformation rules (e.g., position + velocity) to the previous state.<br>
                        If we lived in a perfect universe with no wind, friction, or entropy, the state would be exactly ğ…â‚œ ğ—â‚œâ‚‹â‚.<br>
                        Therefore, we center the bell curve right on top of this physical prediction. It is our "best guess."</p>
                        <p>B. Why is ğâ‚œ the Sigma (Covariance)?<br>
                        In a standard bell curve, Sigma (Ïƒ) defines the width. In this vector equation, ğâ‚œ is the Covariance Matrix.<br>
                        This represents the "Process Noise." It accounts for external realities the physics engine ignored (wind gusts, wheel slips).</p>
                        <p><strong>3. Equation 2: The Sensor (Observation Model)</strong><br>
                        <span class="code-block">ğ˜â‚œ âˆ¼ ğ’©(ğ‡â‚œ ğ—â‚œ, ğ‘â‚œ)</span><br>
                        This equation says: "The Data reading at time ğ‘¡ is a random draw from a Bell Curve centered around the truth."</p>
                        <p>A. Why is the Mean ğ‡â‚œ ğ—â‚œ?<br>
                        ğ‡â‚œ is the "lens" or "mask" matrix. It converts the hidden state (Truth) into what the sensor should theoretically see.<br>
                        If the sensor were perfect, the reading ğ˜â‚œ would equal ğ‡â‚œ ğ—â‚œ exactly.<br>
                        Therefore, we center the probability distribution on this "Ideal Reading."</p>
                        <p>B. Why is ğ‘â‚œ the Sigma (Covariance)?<br>
                        ğ‘â‚œ represents Measurement Noise.<br>
                        This defines the quality of your hardware. A cheap sensor has a high ğ‘ (wide spread, fuzzy data). An expensive, precision sensor has a low ğ‘ (narrow spread, sharp data).</p>
                        <p><strong>4. Summary of Interactions</strong><br>
                        This system allows the Kalman Filter to mathematically weigh two competing uncertainties:<br>
                        The Physics Uncertainty (ğâ‚œ): "How much do I trust my prediction?"<br>
                        The Sensor Uncertainty (ğ‘â‚œ): "How much do I trust my data?"<br>
                        If ğ is large and ğ‘ is small: The math "trusts" the data. The bell curve is wide for the prediction but narrow for the measurement, so the final estimate snaps to the measurement.<br>
                        If ğ is small and ğ‘ is large: The math "trusts" the physics. The data is too noisy, so the filter ignores the data and sticks to the trajectory calculated by ğ…â‚œ ğ—â‚œâ‚‹â‚.</p>
                    </div>
                </div>
            </details>

            <!-- SECTION 12 -->
            <details class="section">
                <summary>4. Kalman Filter</summary>
                <div class="section-content">
                    <div class="subsection">
                        <span class="subsection-title">The Mechanics</span>
                        <p>Think of the Kalman Gain as a dynamic mixing slider that moves automatically at every time step between 0 (Pure Physics) and 1 (Pure Data).</p>
                        <p><strong>1. The Mechanics of the Gain</strong><br>
                        The Gain compares the Predicted Uncertainty (derived from ğ) against the Measurement Uncertainty (ğ‘).<br>
                        <span class="code-block">ğŠâ‚œ â‰ˆ Uncertainty_Predict / (Uncertainty_Predict + Uncertainty_Measure)</span></p>
                    </div>
                    <div class="subsection">
                        <span class="subsection-title">Scenarios</span>
                        <p><strong>2. Scenario A: High ğ, Low ğ‘ (Trust the Data)</strong><br>
                        * The Situation: You are tracking an erratic drone (High Process Noise) with a laser precision tracker (Low Measurement Noise).<br>
                        * The Cycle: During the Predict step, the drone's erratic behavior makes the uncertainty "bubble" expand rapidly.<br>
                        * The Action: The math sees the Prediction is vague but the Sensor is sharp. The Kalman Gain shoots up (close to 1).<br>
                        * The Result: In the Update step, the filter aggressively snaps the state to the measurement, effectively resetting the prediction to match the laser.</p>
                        <p><strong>3. Scenario B: Low ğ, High ğ‘ (Trust the Physics)</strong><br>
                        * The Situation: You are tracking a heavy freight train (Low Process Noise) with a fuzzy analog camera (High Measurement Noise).<br>
                        * The Cycle: During the Predict step, the train's massive inertia means the uncertainty stays tightâ€”we know exactly where it should be.<br>
                        * The Action: The math sees the Sensor is a "shotgun blast" of noise compared to the tight Physics prediction. The Kalman Gain drops (close to 0).<br>
                        * The Result: In the Update step, the filter treats the observation as "static." It barely nudges the state, allowing the smooth physics trajectory to dominate.</p>
                        <p><strong>Summary</strong><br>
                        The Kalman Filter aids the cycle by optimizing the correction. It prevents the system from chasing "ghosts" (sensor noise) while ensuring it doesn't become "blind" to real changes in direction.</p>
                    </div>
                </div>
            </details>
        </div>
    </details>

</div>

</body>
</html>